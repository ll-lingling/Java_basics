
第六部分 MapReduce编程框架

第一节 MapReduce思想

MapReduce思想在生活中处处可见。我们或多或少都曾接触过这种思想。MapReduce的思想核心是 分
而治之 ，

充分利用了并行处理的优势。

即使是发布过论文实现分布式计算的谷歌也只是实现了这种思想，而不是自己原创。

MapReduce任务过程是分为两个处理阶段：

    Map阶段：Map阶段的主要作用是“分”，即把复杂的任务分解为若干个“简单的任务”来 并行 处理。
    Map阶段的这些任务可以并行计算，彼此间没有依赖关系。
    Reduce阶段：Reduce阶段的主要作用是“合”，即对map阶段的结果进行全局汇总。


再次理解MapReduce的思想

第二节 官方WordCount案例源码解析



经过查看分析官方WordCount案例源码我们发现一个统计单词数量的MapReduce程序的代码由三个部
分组成，

    Mapper类
    Reducer类
    运行作业的代码（Driver）


Mapper类继承了org.apache.hadoop.mapreduce.Mapper类重写了其中的map方法，Reducer类继承
了org.apache.hadoop.mapreduce.Reducer类重写了其中的reduce方法。

重写的Map方法作用：map方法其中的逻辑就是用户希望mr程序map阶段如何处理的逻辑；

重写的Reduce方法作用：reduce方法其中的逻辑是用户希望mr程序reduce阶段如何处理的逻辑；

1. Hadoop序列化

为什么进行序列化？

序列化主要是我们通过网络通信传输数据时或者把对象持久化到文件，需要把对象序列化成二进制的结

构。

观察源码时发现自定义Mapper类与自定义Reducer类都有泛型类型约束，比如自定义Mapper有四个形
参类型，但是形参类型并不是常见的java基本类型。

为什么Hadoop要选择建立自己的序列化格式而不使用java自带serializable？

    序列化在分布式程序中非常重要，在Hadoop中，集群中多个节点的进程间的通信是通过RPC（远
    程过程调用：Remote Procedure Call）实现；RPC将消息序列化成二进制流发送到远程节点，远
    程节点再将接收到的二进制数据反序列化为原始的消息，因此RPC往往追求如下特点：
    紧凑:数据更紧凑，能充分利用网络带宽资源
    快速:序列化和反序列化的性能开销更低
    Hadoop使用的是自己的序列化格式Writable,它比java的序列化serialization更紧凑速度更快。一
    个对象使用Serializable序列化后，会携带很多额外信息比如校验信息，Header,继承体系等。


Java基本类型与Hadoop常用序列化类型

    Java基本类型 Hadoop Writable类型


    boolean BooleanWritable


    byte ByteWritable


    int IntWritable


    float FloatWritable


    long LongWritable


    double DoubleWritable


    String Text


    map MapWritable


    array ArrayWritable


第三节 MapReduce编程规范及示例编写

3.1 Mapper类

    用户自定义一个Mapper类继承Hadoop的Mapper类
    Mapper的输入数据是KV对的形式（类型可以自定义）
    Map阶段的业务逻辑定义在map()方法中
    Mapper的输出数据是KV对的形式（类型可以自定义）


注意：map()方法是对输入的一个KV对调用一次！！

3.2 Reducer类

    用户自定义Reducer类要继承Hadoop的Reducer类
    Reducer的输入数据类型对应Mapper的输出数据类型（KV对）
    Reducer的业务逻辑写在reduce()方法中
    Reduce()方法是对相同K的一组KV对调用执行一次


3.3 Driver阶段

创建提交YARN集群运行的Job对象，其中封装了MapReduce程序运行所需要的相关参数入输入数据路
径，输出数据路径等，也相当于是一个YARN集群的客户端，主要作用就是提交我们MapReduce程序运
行。

3.4 WordCount代码实现

3.4.1 需求

在给定的文本文件中统计输出每一个单词出现的总次数

输入数据：wc.txt;

输出：

3.4.2 具体步骤

按照MapReduce编程规范，分别编写Mapper，Reducer，Driver。

1. 新建maven工程

    1. 导入hadoop依赖
    

    apache 2
    clickhouse 2
    hadoop 1
    mapreduce 1
    spark 2
    xiaoming 1
    

    <dependencies>
    

    <dependency>
    <groupId>org.apache.logging.log4j</groupId>
    <artifactId>log4j-core</artifactId>
    <version>2.8.2</version>
    </dependency>
    <dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-common</artifactId>
    <version>2.9.2</version>
    

注意：以上依赖第一次需要联网下载！！

    2. 添加log4j.properties


    </dependency>
    <dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-client</artifactId>
    <version>2.9.2</version>
    </dependency>
    <dependency>
    <groupId>org.apache.hadoop</groupId>
    <artifactId>hadoop-hdfs</artifactId>
    <version>2.9.2</version>
    </dependency>
    </dependencies>


    <!--maven打包插件 -->
    <build>
    <plugins>
    <plugin>
    <artifactId>maven-compiler-plugin</artifactId>
    <version>2.3.2</version>
    <configuration>
    <source>1.8</source>
    <target>1.8</target>
    </configuration>
    </plugin>
    <plugin>
    <artifactId>maven-assembly-plugin </artifactId>
    <configuration>
    <descriptorRefs>
    <descriptorRef>jar-with-dependencies</descriptorRef>
    </descriptorRefs>


    </configuration>
    <executions>
    <execution>
    <id>make-assembly</id>
    <phase>package</phase>
    <goals>
    <goal>single</goal>
    </goals>
    </execution>
    </executions>
    </plugin>
    </plugins>
    </build>


    log4j.rootLogger=INFO, stdout
    log4j.appender.stdout=org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
    log4j.appender.stdout.layout.ConversionPattern=%d %p [%c] - %m%n
    log4j.appender.logfile=org.apache.log4j.FileAppender
    log4j.appender.logfile.File=target/spring.log
    log4j.appender.logfile.layout=org.apache.log4j.PatternLayout
    log4j.appender.logfile.layout.ConversionPattern=%d %p [%c] - %m%n


2. 整体思路梳理（仿照源码）

Map阶段：

    1. map()方法中把传入的数据转为String类型
    2. 根据空格切分出单词
    3. 输出<单词，1>


Reduce阶段：

    1. 汇总各个key(单词)的个数，遍历value数据进行累加
    2. 输出key的总数


Driver

    1. 获取配置文件对象，获取job对象实例
    2. 指定程序jar的本地路径
    3. 指定Mapper/Reducer类
    4. 指定Mapper输出的kv数据类型
    5. 指定最终输出的kv数据类型
    6. 指定job处理的原始数据路径
    7. 指定job输出结果路径
    8. 提交作业


3. 编写Mapper类

    import java.io.IOException;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;
    

    public class WordcountMapper extends Mapper<LongWritable, Text, Text,
    IntWritable>{
    

    Text k = new Text();
    IntWritable v = new IntWritable( 1 );
    

    @Override
    protected void map(LongWritable key, Text value, Context context) throws
    IOException, InterruptedException {
    

    // 1 获取一行
    String line = value.toString();
    

    // 2 切割
    String[] words = line.split(" ");
    

    // 3 输出
    for (String word : words) {
    

    k.set(word);
    

继承的Mapper类型选择新版本API：

4. 编写Reducer类

选择继承的Reducer类

5. 编写Driver驱动类

    context.write(k, v);
    }
    }
    }
    

    import java.io.IOException;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Reducer;
    

    public class WordcountReducer extends Reducer<Text, IntWritable, Text,
    IntWritable>{
    

    int sum;
    IntWritable v = new IntWritable();
    

    @Override
    protected void reduce(Text key, Iterable<IntWritable> values,Context
    context) throws IOException, InterruptedException {
    

    // 1 累加求和
    sum = 0 ;
    for (IntWritable count : values) {
    sum += count.get();
    }
    

    // 2 输出
    v.set(sum);
    context.write(key,v);
    }
    }
    

    import java.io.IOException;
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    
6. 运行任务

1. 本地模式

    直接Idea中运行驱动类即可
    idea运行需要传入参数：
    

    选择editconfiguration
    

    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    

    public class WordcountDriver {
    

    public static void main(String[] args) throws IOException,
    ClassNotFoundException, InterruptedException {
    

    // 1 获取配置信息以及封装任务
    Configuration configuration = new Configuration();
    Job job = Job.getInstance(configuration);
    

    // 2 设置jar加载路径
    job.setJarByClass(WordcountDriver.class);
    

    // 3 设置map和reduce类
    job.setMapperClass(WordcountMapper.class);
    job.setReducerClass(WordcountReducer.class);
    

    // 4 设置map输出
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(IntWritable.class);
    

    // 5 设置最终输出kv类型
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    

    // 6 设置输入和输出路径
    FileInputFormat.setInputPaths(job, new Path(args[ 0 ]));
    FileOutputFormat.setOutputPath(job, new Path(args[ 1 ]));
    

    // 7 提交
    boolean result = job.waitForCompletion(true);
    

    System.exit(result? 0 : 1 );
    }
    }
    

    在program arguments设置参数
    

运行结束，去到输出结果路径查看结果

    注意本地idea运行mr任务与集群没有任何关系，没有提交任务到yarn集群，是在本地使用多线程
    方式模拟的mr的运行。


2. Yarn集群模式

    把程序打成jar包，改名为wc.jar;上传到Hadoop集群
    选择合适的Jar包
    

准备原始数据文件，上传到HDFS的路径，不能是本地路径，因为跨节点运行无法获取数

据！！

    启动Hadoop集群（Hdfs,Yarn）
    使用Hadoop 命令提交任务运行


    hadoop jar  wc.jar com.lagou.wordcount.WordcountDriver
    /user/lagou/input /user/lagou/output


Yarn集群任务运行成功展示图

第四节 序列化Writable接口

基本序列化类型往往不能满足所有需求，比如在Hadoop框架内部传递一个自定义bean对象，那么该对
象就需要实现Writable序列化接口。

4.1 实现Writable序列化步骤如下

    1. 必须实现Writable接口
    2. 反序列化时，需要反射调用空参构造函数，所以必须有空参构造


3. 重写序列化方法

4. 重写反序列化方法

5. 反序列化的字段顺序和序列化字段的顺序必须完全一致

    6. 方便展示结果数据，需要重写bean对象的toString()方法，可以自定义分隔符
    7. 如果自定义Bean对象需要放在Mapper输出KV中的K,则该对象还需实现Comparable接口，因为因
    为MapReduce框中的Shuffle过程要求对key必须能排序！！
    排序内容专门案例讲解！！
    

    public CustomBean() {
    super();
    }
    

    @Override
    public void write(DataOutput out) throws IOException {
    ....
    }
    

    @Override
    public void readFields(DataInput in) throws IOException {
    ....
    }
    

4.2 Writable接口案例

1. 需求

统计每台智能音箱设备内容播放时长

原始日志格式

输出结果

2. 编写MapReduce程序

    1. 创建SpeakBean对象
    

    @Override
    public int compareTo(CustomBean o) {
    // 自定义排序规则
    return this.num > o.getNum()? - 1 : 1 ;
    }
    

    001 001577c3 kar_890809 120.196.100.99 1116 954
    200
    日志id 设备id appkey(合作硬件厂商) 网络ip 自有内容时长(秒) 第三方内
    容时长(秒) 网络状态码
    

    001577c3 11160 9540 20700
    设备id 自有内容时长(秒) 第三方内容时长(秒) 总时长
    

    package com.lagou.hdfs;
    

    import org.apache.hadoop.io.Writable;
    

    import java.io.DataInput;
    import java.io.DataOutput;
    import java.io.IOException;
    // 1 实现writable接口
    public class SpeakBean implements Writable {
    

    private long selfDuration;
    private long thirdPartDuration;
    private long sumDuration;
    //2 反序列化时，需要反射调用空参构造函数，所以必须有
    public SpeakBean() {
    }
    

    public SpeakBean(long selfDuration, long thirdPartDuration) {
    this.selfDuration = selfDuration;
    this.thirdPartDuration = thirdPartDuration;
    this.sumDuration=this.selfDuration+this.thirdPartDuration;
    }
    

    //3 写序列化方法
    public void write(DataOutput out) throws IOException {
    
2. 编写Mapper类

    out.writeLong(selfDuration);
    out.writeLong(thirdPartDuration);
    out.writeLong(sumDuration);
    }
    //4 反序列化方法
    //5 反序列化方法读顺序必须和写序列化方法的写顺序必须一致
    public void readFields(DataInput in) throws IOException {
    this.selfDuration = in.readLong();
    this.thirdPartDuration = in.readLong();
    this.sumDuration = in.readLong();
    }
    // 6 编写toString方法，方便后续打印到文本
    @Override
    public String toString() {
    return selfDuration +
    "\t" + thirdPartDuration +
    "\t" + sumDuration ;
    }
    

    public long getSelfDuration() {
    return selfDuration;
    }
    

    public void setSelfDuration(long selfDuration) {
    this.selfDuration = selfDuration;
    }
    

    public long getThirdPartDuration() {
    return thirdPartDuration;
    }
    

    public void setThirdPartDuration(long thirdPartDuration) {
    this.thirdPartDuration = thirdPartDuration;
    }

    public long getSumDuration() {
    return sumDuration;
    }

    public void setSumDuration(long sumDuration) {
    this.sumDuration = sumDuration;
    }

    public void set(long selfDuration, long thirdPartDuration) {
    this.selfDuration = selfDuration;
    this.thirdPartDuration = thirdPartDuration;
    this.sumDuration=this.selfDuration+this.thirdPartDuration;
    }
    }

    package com.lagou.hdfs;

    import org.apache.hadoop.io.LongWritable;

3. 编写Reducer

    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;

    import java.io.IOException;

    public class SpeakDurationMapper extends Mapper<LongWritable, Text, Text,
    SpeakBean> {
    SpeakBean v = new SpeakBean();
    Text k = new Text();

    @Override
    protected void map(LongWritable key, Text value, Context context)
    throws IOException, InterruptedException {

    // 1 获取一行
    String line = value.toString();

    // 2 切割字段
    String[] fields = line.split("\t");
    

    // 3 封装对象
    // 取出设备id
    String deviceId = fields[ 1 ];
    

    // 取出自有和第三方时长数据
    long selfDuration = Long.parseLong(fields[fields.length - 3 ]);
    long thirdPartDuration = Long.parseLong(fields[fields.length - 2 ]);
    

    k.set(deviceId);
    v.set(selfDuration, thirdPartDuration);
    

    // 4 写出
    context.write(k, v);
    }
    }
    

    package com.lagou.hdfs;
    

    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Reducer;
    

    import java.io.IOException;
    

    public class SpeakDurationReducer extends Reducer<Text, SpeakBean, Text,
    SpeakBean> {
    @Override
    protected void reduce(Text key, Iterable<SpeakBean> values, Context
    context)throws IOException, InterruptedException {
    

    long self_Duration = 0 ;
    long thirdPart_Duration = 0 ;
    
4. 编写驱动

    // 1 遍历所用bean，将其中的自有，第三方时长分别累加
    for (SpeakBean sb : values) {
    self_Duration += sb.getSelfDuration();
    thirdPart_Duration += sb.getThirdPartDuration();
    }
    

    // 2 封装对象
    SpeakBean resultBean = new SpeakBean(self_Duration,
    thirdPart_Duration);
    

    // 3 写出
    context.write(key, resultBean);
    }
    }
    

    package com.lagou.hdfs;
    

    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    

    import java.io.IOException;
    

    public class SpeakerDriver {
    public static void main(String[] args) throws IllegalArgumentException,
    IOException, ClassNotFoundException, InterruptedException {
    

    // 输入输出路径需要根据自己电脑上实际的输入输出路径设置
    args = new String[] { "e:/input/input", "e:/output1" };
    

    // 1 获取配置信息，或者job对象实例
    Configuration configuration = new Configuration();
    Job job = Job.getInstance(configuration);
    

    // 6 指定本程序的jar包所在的本地路径
    job.setJarByClass(SpeakerDriver.class);
    

    // 2 指定本业务job要使用的mapper/Reducer业务类
    job.setMapperClass(SpeakDurationMapper.class);
    job.setReducerClass(SpeakDurationReducer.class);
    

    // 3 指定mapper输出数据的kv类型
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(SpeakBean.class);
    

    // 4 指定最终输出的数据的kv类型
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(SpeakBean.class);
    

mr编程技巧总结

    结合业务设计Map输出的key和v，利用key相同则去往同一个reduce的特点！！
    map()方法中获取到只是一行文本数据尽量不做聚合运算
    reduce()方法的参数要清楚含义


第五节 MapReduce原理分析

5.1 MapTask运行机制详解

MapTask流程

详细步骤：

    1. 首先，读取数据组件InputFormat（默认TextInputFormat）会通过getSplits方法对输入目录中文
    件进行逻辑切片规划得到splits，有多少个split就对应启动多少个MapTask。split与block的对应关
    系默认是一对一。
    2. 将输入文件切分为splits之后，由RecordReader对象（默认LineRecordReader）进行读取，以\n
    作为分隔符，读取一行数据，返回<key，value>。Key表示每行首字符偏移值，value表示这一行
    文本内容。
    3. 读取split返回<key,value>，进入用户自己继承的Mapper类中，执行用户重写的map函数。
    RecordReader读取一行这里调用一次。
    4. map逻辑完之后，将map的每条结果通过context.write进行collect数据收集。在collect中，会先
    对其进行分区处理，默认使用HashPartitioner。


MapReduce提供Partitioner接口，它的作用就是根据key或value及reduce的数量来决定当前的这对
输出数据最终应该交由哪个reduce task处理。默认对key hash后再以reduce task数量取模。默认的
取模方式只是为了平均reduce的处理能力，如果用户自己对Partitioner有需求，可以订制并设置到
job上。

    5. 接下来，会将数据写入内存，内存中这片区域叫做环形缓冲区，缓冲区的作用是批量收集map结
    果，减少磁盘IO的影响。我们的key/value对以及Partition的结果都会被写入缓冲区。当然写入之
    前，key与value值都会被序列化成字节数组。


    // 5 指定job的输入原始文件所在目录
    FileInputFormat.setInputPaths(job, new Path(args[ 0 ]));
    FileOutputFormat.setOutputPath(job, new Path(args[ 1 ]));


    // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行
    boolean result = job.waitForCompletion(true);
    System.exit(result? 0 : 1 );
    }
    }


    环形缓冲区其实是一个数组，数组中存放着key、value的序列化数据和key、value的元数据信
    息，包括partition、key的起始位置、value的起始位置以及value的长度。环形结构是一个抽象概
    念。
    缓冲区是有大小限制，默认是100MB。当map task的输出结果很多时，就可能会撑爆内存，所以
    需要在一定条件下将缓冲区中的数据临时写入磁盘，然后重新利用这块缓冲区。这个从内存往磁盘
    写数据的过程被称为Spill，中文可译为溢写。这个溢写是由单独线程来完成，不影响往缓冲区写
    map结果的线程。溢写线程启动时不应该阻止map的结果输出，所以整个缓冲区有个溢写的比例
    spill.percent。这个比例默认是0.8，也就是当缓冲区的数据已经达到阈值（buffer size * spill
    percent = 100MB * 0.8 = 80MB），溢写线程启动，锁定这80MB的内存，执行溢写过程。Map
    task的输出结果还可以往剩下的20MB内存中写，互不影响。


6 、当溢写线程启动后，需要对这80MB空间内的key做排序(Sort)。排序是MapReduce模型默认的行为!

    如果job设置过Combiner，那么现在就是使用Combiner的时候了。将有相同key的key/value对的
    value加起来，减少溢写到磁盘的数据量。Combiner会优化MapReduce的中间结果，所以它在整
    个模型中会多次使用。
    那哪些场景才能使用Combiner呢？从这里分析，Combiner的输出是Reducer的输入，Combiner
    绝不能改变最终的计算结果。Combiner只应该用于那种Reduce的输入key/value与输出key/value
    类型完全一致，且不影响最终结果的场景。比如累加，最大值等。Combiner的使用一定得慎重，
    如果用好，它对job执行效率有帮助，反之会影响reduce的最终结果。


    7. 合并溢写文件：每次溢写会在磁盘上生成一个临时文件（写之前判断是否有combiner），如果
    map的输出结果真的很大，有多次这样的溢写发生，磁盘上相应的就会有多个临时文件存在。当
    整个数据处理结束之后开始对磁盘中的临时文件进行merge合并，因为最终的文件只有一个，写入
    磁盘，并且为这个文件提供了一个索引文件，以记录每个reduce对应数据的偏移量。


至此map整个阶段结束!!

MapTask的一些配置

官方参考地址

5.2 MapTask的并行度

    1. MapTask并行度思考


MapTask的并行度决定Map阶段的任务处理并发度，从而影响到整个Job的处理速度。

思考：MapTask并行任务是否越多越好呢？哪些因素影响了MapTask并行度？

    2. MapTask并行度决定机制


数据块：Block是HDFS物理上把数据分成一块一块。

切片：数据切片只是在逻辑上对输入进行分片，并不会在磁盘上将其切分成片进行存储。

    https://hadoop.apache.org/docs/r2.9.2/hadoop-mapreduce-client/hadoop-mapreduce-
    client-core/mapred-default.xml


5.2.1 切片机制源码阅读

默认就是128M；

MapTask并行度是不是越多越好呢？

答案不是，如果一个文件仅仅比128M大一点点也被当成一个split来对待，而不是多个split.

MR框架在并行运算的同时也会消耗更多资源，并行度越高资源消耗也越高，假设129M文件分为两个分

片，一个是128M，一个是1M；

对于1M的切片的Maptask来说，太浪费资源。

129M的文件在Hdfs存储的时候会不会切成两块？

5.3 ReduceTask 工作机制

Reduce大致分为 copy、sort、reduce 三个阶段，重点在前两个阶段。copy阶段包含一个
eventFetcher来获取已完成的map列表，由Fetcher线程去copy数据，在此过程中会启动两个merge线
程，分别为inMemoryMerger和onDiskMerger，分别将内存中的数据merge到磁盘和将磁盘中的数据
进行merge。待数据copy完成之后，copy阶段就完成了，开始进行sort阶段，sort阶段主要是执行
finalMerge操作，纯粹的sort阶段，完成之后就是reduce阶段，调用用户定义的reduce函数进行处理。

详细步骤

    Copy阶段，简单地拉取数据。Reduce进程启动一些数据copy线程(Fetcher)，通过HTTP方式请求
    maptask获取属于自己的文件。
    Merge阶段。这里的merge如map端的merge动作，只是数组中存放的是不同map端copy来的数
    值。Copy过来的数据会先放入内存缓冲区中，这里的缓冲区大小要比map端的更为灵活。merge
    有三种形式：内存到内存；内存到磁盘；磁盘到磁盘。默认情况下第一种形式不启用。当内存中的
    数据量到达一定阈值，就启动内存到磁盘的merge。与map 端类似，这也是溢写的过程，这个过
    程中如果你设置有Combiner，也是会启用的，然后在磁盘中生成了众多的溢写文件。第二种
    merge方式一直在运行，直到没有map端的数据时才结束，然后启动第三种磁盘到磁盘的merge
    方式生成最终的文件。
    合并排序。把分散的数据合并成一个大的数据后，还会再对合并后的数据排序。
    对排序后的键值对调用reduce方法，键相等的键值对调用一次reduce方法，每次调用会产生零个
    或者多个键值对，最后把这些输出的键值对写入到HDFS文件中。


5.4 ReduceTask并行度

ReduceTask的并行度同样影响整个Job的执行并发度和执行效率，但与MapTask的并发数由切片数决定
不同，ReduceTask数量的决定是可以直接手动设置：

注意事项

    1. ReduceTask=0，表示没有Reduce阶段，输出文件数和MapTask数量保持一致；
    2. ReduceTask数量不设置默认就是一个，输出文件数量为 1 个；
    3. 如果数据分布不均匀，可能在Reduce阶段产生倾斜；


// 默认值是 1 ，手动设置为 4

    job.setNumReduceTasks( 4 );


5.5 Shuffle机制

map阶段处理的数据如何传递给reduce阶段，是MapReduce框架中最关键的一个流程，这个流程就叫
shuffle。

shuffle: 洗牌、发牌——（核心机制：数据分区，排序，分组，combine，合并等过程）

5.5.1 MapReduce的分区与reduceTask的数量

在MapReduce中，通过我们指定分区，会将同一个分区的数据发送到同一个reduce当中进行处理(默认
是 key相同去往同个分区 )，例如我们为了数据的统计，我们可以把一批类似的数据发送到同一个reduce
当中去，在同一个reduce当中统计相同类型的数据，

如何才能保证相同key的数据去往同个reduce呢？只需要保证相同key的数据分发到同个分区即可。结
合以上原理分析我们知道MR程序shuffle机制默认就是这种规则！！

1. 分区源码

翻阅源码验证以上规则，MR程序默认使用的HashPartitioner，保证了相同的key去往同个分区！！

2. 自定义分区

实际生产中需求变化多端，默认分区规则往往不能满足需求，需要结合业务逻辑来灵活控制分区规则以

及分区数量！！

如何制定自己需要的分区规则？

具体步骤

    1. 自定义类继承Partitioner，重写getPartition()方法
    2. 在Driver驱动中，指定使用自定义Partitioner
    3. 在Driver驱动中，要根据自定义Partitioner的逻辑设置相应数量的ReduceTask数量。


需求 按照不同的appkey把记录输出到不同的分区中

原始日志格式

输出结果

需求分析

面对业务需求，结合mr的特点，来设计map输出的kv,以及reduce输出的kv数据。

一个ReduceTask对应一个输出文件，因为在shuffle机制中每个reduceTask拉取的都是某一个分区的数
据，一个分区对应一个输出文件。

结合appkey的前缀相同的特点，同时不能使用默认分区规则，而是使用自定义分区器，只要appkey前
缀相同则数据进入同个分区。

整体思路

Mapper

    1. 读取一行文本，按照制表符切分
    2. 解析出appkey字段，其余数据封装为PartitionBean对象（实现序列化Writable接口）


    001 001577c3 kar_890809 120.196.100.99 1116 954
    200
    日志id 设备id appkey(合作硬件厂商) 网络ip 自有内容时长(秒) 第三方内
    容时长(秒) 网络状态码


    根据appkey把不同厂商的日志数据分别输出到不同的文件中


    3. 设计map()输出的kv,key-->appkey(依靠该字段完成分区)，PartitionBean对象作为Value输出


Partition

自定义分区器，实现按照appkey字段的前缀来区分所属分区

Reduce

    1. reduce()正常输出即可，无需进行聚合操作


Driver

    1. 在原先设置job属性的同时增加设置使用自定义分区器
    2. 注意设置ReduceTask的数量（与分区数量保持一致）


Mapper

PartitionBean

    package com.lagou.mr.partition;


    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;
    
    import java.io.IOException;
    
    public class PartitionMapper extends Mapper<LongWritable, Text, Text,
    PartitionBean> {
    final PartitionBean bean = new PartitionBean();
    Text text = new Text();
    
    @Override
    protected void map(LongWritable key, Text value, Context context) throws
    IOException, InterruptedException {
    final String[] fields = value.toString().split("\t");
    String appkey = fields[ 2 ];
    bean.setId(fields[ 0 ]);
    bean.setDeviceId(fields[ 1 ]);
    bean.setAppkey(appkey);
    bean.setIp(fields[ 3 ]);
    bean.setSelfDuration(Long.parseLong(fields[ 4 ]));
    bean.setThirdPartDuration(Long.parseLong(fields[ 5 ]));
    bean.setStatus(fields[ 6 ]);
    text.set(appkey);
    context.write(text, bean);
    }
    }
    
    package com.lagou.mr.partition;
    
    import org.apache.hadoop.io.Writable;
    
    import java.io.DataInput;
    import java.io.DataOutput;
    import java.io.IOException;

public class PartitionBean implements Writable {
//定义属性
private String id; //日志id
private String deviceId;//设备id
private String appkey;//appkey合作硬件厂商id
private String ip;//ip地址
private Long selfDuration;//自有内容时长
private Long thirdPartDuration;//第三方内容时长
private String status;//状态码

public PartitionBean() {
}

public PartitionBean(String id, String deviceId, String appkey, String ip,
Long selfDuration, Long thirdPartDuration, String status) {
this.id = id;
this.deviceId = deviceId;
this.appkey = appkey;
this.ip = ip;
this.selfDuration = selfDuration;
this.thirdPartDuration = thirdPartDuration;
this.status = status;
}

public String getId() {
return id;
}

public void setId(String id) {
this.id = id;
}

public String getDeviceId() {
return deviceId;
}

public void setDeviceId(String deviceId) {
this.deviceId = deviceId;
}

public String getAppkey() {
return appkey;
}

public void setAppkey(String appkey) {
this.appkey = appkey;
}

public String getIp() {
return ip;
}

public void setIp(String ip) {
this.ip = ip;
}

public Long getSelfDuration() {

return selfDuration;
}

public void setSelfDuration(Long selfDuration) {
this.selfDuration = selfDuration;
}

public Long getThirdPartDuration() {
return thirdPartDuration;
}

public void setThirdPartDuration(Long thirdPartDuration) {
this.thirdPartDuration = thirdPartDuration;
}

public String getStatus() {
return status;
}

public void setStatus(String status) {
this.status = status;
}

//序列化
@Override
public void write(DataOutput out) throws IOException {
out.writeUTF(id);
out.writeUTF(deviceId);
out.writeUTF(appkey);
out.writeUTF(ip);
out.writeLong(selfDuration);
out.writeLong(thirdPartDuration);
out.writeUTF(status);
}

//反序列化
@Override
public void readFields(DataInput in) throws IOException {
this.id = in.readUTF();
this.deviceId=in.readUTF();
this.appkey=in.readUTF();
this.ip=in.readUTF();
this.selfDuration=in.readLong();
this.thirdPartDuration=in.readLong();
this.status=in.readUTF();
}

@Override
public String toString() {
return id + '\t' +
deviceId + '\t' +
appkey + '\t' +
ip + '\t' +
selfDuration +'\t'+
thirdPartDuration +
'\t' + status ;
}
}

CustomPartitioner

PartitionReducer

PartitionDriver

    package com.lagou.mr.partition;


    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Partitioner;


    public class CustomPartitioner extends Partitioner<Text,PartitionBean> {
    @Override
    public int getPartition(Text text, PartitionBean partitionBean, int
    numPartitions) {
    int partition= 0 ;
    final String appkey = text.toString();


    if(appkey.equals("kar")){
    partition= 1 ;
    }else if(appkey.equals("pandora")){
    partition= 2 ;
    }else{
    partition= 0 ;
    }
    return partition;
    }
    }


    package com.lagou.mr.partition;


    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Reducer;


    import java.io.IOException;


    public class PartitionReducer extends Reducer<Text, PartitionBean, NullWritable,
    PartitionBean> {


    @Override
    protected void reduce(Text key, Iterable<PartitionBean> values, Context
    context)
    throws IOException, InterruptedException {
    for (PartitionBean bean : values) {
    context.write(NullWritable.get(), bean);
    }
    }
    }


总结

    1. 自定义分区器时最好保证分区数量与reduceTask数量保持一致；
    2. 如果分区数量不止 1 个，但是reduceTask数量 1 个，此时只会输出一个文件。
    3. 如果reduceTask数量大于分区数量，但是输出多个空文件
    4. 如果reduceTask数量小于分区数量，有可能会报错。


    package com.lagou.mr.partition;


    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


    import java.io.IOException;


    public class PartitionDriver {
    public static void main(String[] args) throws IOException,
    ClassNotFoundException, InterruptedException {
    final Configuration conf = new Configuration();
    final Job job = Job.getInstance(conf);


    job.setJarByClass(PartitionDriver.class);


    job.setMapperClass(PartitionMapper.class);
    job.setReducerClass(PartitionReducer.class);


    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(PartitionBean.class);
    job.setOutputKeyClass(NullWritable.class);
    job.setOutputValueClass(PartitionBean.class);


    job.setPartitionerClass(CustomPartitioner.class);
    job.setNumReduceTasks( 5 );
    FileInputFormat.setInputPaths(job, new Path("e:/speak.data"));
    FileOutputFormat.setOutputPath(job, new Path("e:/partition/output"));


    final boolean flag = job.waitForCompletion(true);
    System.exit(flag? 0 : 1 );
    }
    }


5.

5.5.2 MapReduce中的Combiner

combiner运行机制：

    1. Combiner是MR程序中Mapper和Reducer之外的一种组件
    2. Combiner组件的父类就是 Reducer
    3. Combiner和reducer的区别在于运行的位置
    4. Combiner是在每一个maptask所在的节点运行;
    5. Combiner的意义就是对每一个maptask的输出进行局部汇总，以减小 网络传输量 。
    6. Combiner能够应用的前提是不能影响最终的业务逻辑，此外，Combiner的输出kv应该跟reducer
    的输入kv类型要对应起来。
    举例说明
    假设一个计算平均值的MR任务
    Map阶段
    2 个MapTask
    MapTask1输出数据：10,5,15 如果使用Combiner:(10+5+15)/3=10
    MapTask2输出数据：2,6 如果使用Combiner:(2+6)/2=4
    Reduce阶段汇总
    （10+4）/2=7
    而正确结果应该是


（10+5+15+2+6）/5=7.6

    自定义Combiner实现步骤
    自定义一个Combiner 继承Reducer ，重写Reduce方法
    在驱动(Driver)设置使用Combiner（默认是不适用Combiner组件）


1. 改造WordCount程序

在驱动(Driver)设置使用Combiner

验证结果

观察程序运行日志

    package com.lagou.mr.wc;


    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.mapreduce.Reducer;


    import javax.xml.soap.Text;
    import java.io.IOException;


    public class WordCountCombiner extends Reducer<Text,
    IntWritable,Text,IntWritable> {


    IntWritable total = new IntWritable();
    @Override
    protected void reduce(Text key, Iterable<IntWritable> values, Context
    context) throws IOException, InterruptedException {
    //2 遍历key对应的values，然后累加结果
    int sum = 0 ;
    for (IntWritable value : values) {
    int i = value.get();
    sum += 1 ;
    }
    // 3 直接输出当前key对应的sum值，结果就是单词出现的总次数
    total.set(sum);
    context.write(key, total);
    }
    }


    job.setCombinerClass(WordcountCombiner.class);


如果直接使用WordCountReducer作为Combiner使用是否可以？

5.6 MapReduce中的排序

排序是MapReduce框架中最重要的操作之一 。

MapTask和ReduceTask均会对数据按照 key 进行排序。该操作属于Hadoop的 默认 行为。任何应用程序
中的数据均会被排序，而不管逻辑.上是否需要。默认排序是按照 字典顺序排序 ，且实现该排序的方法是
快速排序 。

    MapTask
    它会将处理的结果暂时放到环形缓冲区中，当环形缓冲区使用率达到一定阈值后，再对缓冲
    区中的数据进行一次快速排序，并将这些有序数据溢写到磁盘上，
    溢写完毕后，它会对磁盘上所有文件进行归并排序。
    ReduceTask 当所有数据拷贝完毕后，ReduceTask统-对内存和磁盘上的所有数据进行一次归并排
    序。
    
    1. 部分排序.
    MapReduce根据输入记录的键对数据集排序。保证输出的每个 文件内部有序 。
    2. 全排序
    最终输出结果只有 一个文件 ，且文件内部有序。实现方式是只设置- -个ReduceTask。但该方法在
    处理大型文件时效率极低，因为- -台机器处理所有文件，完全丧失了MapReduce所提供的并行架
    构。
    3. 辅助排序: ( GroupingComparator分组)
    在Reduce端对key进行分组。应用于:在接收的key为bean对象时，想让一个或几个字段相同(全部
    字段比较不相同)的key进入到同一个reduce方法时，可以采用分组排序。
    4. 二次排序.
    在自定义排序过程中，如果compareTo中的判断条件为两个即为二次排序。

5.6.1 WritableComparable

Bean对象如果作为Map输出的key时，需要实现WritableComparable接口并重写compareTo方法指定
排序规则

1 全排序

基于统计的播放时长案例的输出结果对总时长进行排序

实现全局排序只能设置一个ReduceTask!!

    直接使用Reducer作为Combiner组件来使用是可以的！！

播放时长案例输出结果

需求分析

如何设计map()方法输出的key,value

MR框架中shuffle阶段的排序是默认行为，不管你是否需要都会进行排序。

key:把所有字段封装成为一个bean对象，并且指定bean对象作为key输出，如果作为key输出，需要实
现排序接口，指定自己的排序规则；

具体步骤

Mapper

    1. 读取结果文件，按照制表符进行切分
    2. 解析出相应字段封装为SpeakBean
    3. SpeakBean实现WritableComparable接口重写compareTo方法
    4. map()方法输出kv;key-->SpeakBean,value-->NullWritable.get()


Reducer

    1. 循环遍历输出


Mapper代码

    00fdaf3 33180 33420 00fdaf3 66600
    00wersa4 30689 35191 00wersa4 65880
    0a0fe2 43085 44254 0a0fe2 87339
    0ad0s7 31702 29183 0ad0s7 60885
    0sfs01 31883 29101 0sfs01 60984
    a00df6s 33239 36882 a00df6s 70121
    adfd00fd5 30727 31491 adfd00fd5 62218


    package com.lagou.mr.WritableComparable;


    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;


    import java.io.IOException;


    public class SortMapper extends Mapper<LongWritable, Text, SpeakBeanSort,
    NullWritable> {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws
    IOException, InterruptedException {
    final String[] arr = value.toString().split("\t");
    final SpeakBeanSort bean = new SpeakBeanSort(Long.parseLong(arr[ 1 ]),
    Long.parseLong(arr[ 2 ]), arr[ 0 ]);
    context.write(bean, NullWritable.get());
    }
    }


Reducer

Bean对象实现WritableComparable接口

    package com.lagou.mr.WritableComparable;


    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.mapreduce.Reducer;


    import java.io.IOException;


    public class SortReducer extends Reducer<SpeakBeanSort, NullWritable,
    NullWritable, SpeakBeanSort> {
    @Override
    protected void reduce(SpeakBeanSort key, Iterable<NullWritable> values,
    Context context) throws IOException, InterruptedException {
    for (NullWritable value : values) {
    context.write(value, key);
    }
    }
    }


    package com.lagou.mr.WritableComparable;


    import org.apache.hadoop.io.WritableComparable;


    import java.io.DataInput;
    import java.io.DataOutput;
    import java.io.IOException;


    public class SpeakBeanSort implements WritableComparable<SpeakBeanSort> {


    //定义属性
    private Long selfDuration;//自有内容时长
    private Long thirdPartDuration;//第三方内容时长
    private String deviceId;//设备id
    private Long sumDuration;//总时长


    //准备一个空参构造


    public SpeakBeanSort() {
    }


//序列化方法:就是把内容输出到网络或者文本中

    @Override
    public void write(DataOutput out) throws IOException {
    out.writeLong(selfDuration);
    out.writeLong(thirdPartDuration);
    out.writeUTF(deviceId);
    out.writeLong(sumDuration);
    }
    
    //有参构造

public SpeakBeanSort(Long selfDuration, Long thirdPartDuration, String
deviceId) {
this.selfDuration = selfDuration;
this.thirdPartDuration = thirdPartDuration;
this.deviceId = deviceId;
this.sumDuration = this.selfDuration + this.thirdPartDuration;
}

//反序列化方法
@Override
public void readFields(DataInput in) throws IOException {
this.selfDuration = in.readLong();//自由时长
this.thirdPartDuration = in.readLong();//第三方时长
this.deviceId = in.readUTF();//设备id
this.sumDuration = in.readLong();//总时长
}

public Long getSelfDuration() {
return selfDuration;
}

public void setSelfDuration(Long selfDuration) {
this.selfDuration = selfDuration;
}

public Long getThirdPartDuration() {
return thirdPartDuration;
}

public void setThirdPartDuration(Long thirdPartDuration) {
this.thirdPartDuration = thirdPartDuration;
}

public String getDeviceId() {
return deviceId;
}

public void setDeviceId(String deviceId) {
this.deviceId = deviceId;
}

public Long getSumDuration() {
return sumDuration;
}

public void setSumDuration(Long sumDuration) {
this.sumDuration = sumDuration;
}

//为了方便观察数据，重写toString()方法

@Override
public String toString() {
return
selfDuration +
"\t" + thirdPartDuration +

Driver

    "\t" + deviceId + "\t" + sumDuration;
    }


    //重写compareTo方法，此处是比较一个字段如果比较 2 个字段就是二次排序
    @Override
    public int compareTo(SpeakBeanSort o) {
    int result;


    // 按照总流量大小，倒序排列
    if (sumDuration > o.getSumDuration()) {
    result = - 1 ;
    }else if (sumDuration < o.getSumDuration()) {
    result = 1 ;
    }else {
    result = 0 ;
    }


    return result;
    }
    }


    package com.lagou.mr.WritableComparable;


    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


    import java.io.IOException;


    public class SortDriver {
    public static void main(String[] args) throws ClassNotFoundException,
    IOException, InterruptedException {


    // 1 获取配置信息，或者job对象实例
    Configuration configuration = new Configuration();
    Job job = Job.getInstance(configuration);


    // 2 指定本程序的jar包所在的本地路径
    job.setJarByClass(SortDriver.class);


    // 3 指定本业务job要使用的mapper/Reducer业务类
    job.setMapperClass(SortMapper.class);
    job.setReducerClass(SortReducer.class);


    // 4 指定mapper输出数据的kv类型
    job.setMapOutputKeyClass(SpeakBeanSort.class);
    job.setMapOutputValueClass(NullWritable.class);


    // 5 指定最终输出的数据的kv类型


总结

    1. 自定义对象作为Map的key输出时，需要实现WritableComparable接口，排序：重写
    compareTo()方法，序列以及反序列化方法
    2. 再次理解reduce()方法的参数；reduce()方法是map输出的kv中key相同的kv中的v组成一个集合调
    用一次reduce()方法，选择遍历values得到所有的key.
    3. 默认reduceTask数量是 1 个；
    4. 对于全局排序需要保证只有一个reduceTask!!


2 分区排序（默认的分区规则，区内有序）

5.6.2 GroupingComparator

GroupingComparator是mapreduce当中reduce端的一个功能组件，主要的作用是决定哪些数据作为
一组，调用一次reduce的逻辑，默认是每个不同的key，作为多个不同的组，每个组调用一次reduce逻
辑，我们可以自定义GroupingComparator实现不同的key作为同一个组，调用一次reduce逻辑。

    订单id 商品id 成交金额


    Order_0000001 Pdt_01 222.8


    Order_0000001 Pdt_05 25.8


    Order_0000002 Pdt_03 522.8


    Order_0000002 Pdt_04 122.4


    Order_0000002 Pdt_05 722.4


    Order_0000003 Pdt_01 232.8


1. 需求

原始数据

需要求出每一个订单中成交金额最大的一笔交易。

2. 实现思路

    Mapper
    读取一行文本数据，切分出每个字段；
    订单id和金额封装为一个Bean对象，Bean对象的排序规则指定为先按照订单Id排序，订单Id
    相等再按照金额 降序 排；
    map()方法输出kv;key-->bean对象，value-->NullWritable.get()；
    

    job.setOutputKeyClass(NullWritable.class);
    job.setOutputValueClass(SpeakBeanSort.class);
    

    // 6 指定job的输入原始文件所在目录
    FileInputFormat.setInputPaths(job, new Path(args[ 0 ]));
    FileOutputFormat.setOutputPath(job, new Path(args[ 1 ]));
    

    // 7 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行
    boolean result = job.waitForCompletion(true);
    System.exit(result? 0 : 1 );
    }
    }
    

    Shuffle
    指定分区器，保证相同订单id的数据去往同个分区（自定义分区器）
    

    指定GroupingComparator，分组规则指定只要订单Id相等则认为属于同一组；
    

Reduce

    每个reduce()方法写出一组key的第一个


参考代码

    OrderBean
    OrderBean定义两个字段，一个字段是orderId，第二个字段是金额（ 注意金额一定要使用
    Double或者DoubleWritable类型，否则没法按照金额顺序排序 ）
    排序规则指定为先按照订单Id排序，订单Id相等再按照金额 降序 排！！


    package com.lagou.mr.group;


    import org.apache.hadoop.io.WritableComparable;


    import java.io.DataInput;
    import java.io.DataOutput;
    import java.io.IOException;


    public class OrderBean implements WritableComparable<OrderBean> {
    private String orderId;
    private Double price;


    @Override
    public int compareTo(OrderBean o) {
    //比较订单id的排序顺序
    int i = this.orderId.compareTo(o.orderId);
    if (i == 0 ) {
    //如果订单id相同，则比较金额，金额大的排在前面
    i = - this.price.compareTo(o.price);
    }
    return i;


    }


    @Override
    public void write(DataOutput out) throws IOException {
    out.writeUTF(orderId);
    out.writeDouble(price);
    }


    @Override
    public void readFields(DataInput in) throws IOException {
    this.orderId = in.readUTF();
    this.price = in.readDouble();
    }


    public OrderBean() {
    }


    public OrderBean(String orderId, Double price) {
    this.orderId = orderId;


自定义分区器

保证ID相同的订单去往同个分区最终去往同一个Reduce中

自定义GroupingComparator

保证id相同的订单进入一个分组中，进入分组的数据已经是按照金额降序排序。reduce()方法取出
第一个即是金额最高的交易

    this.price = price;
    }


    public String getOrderId() {
    return orderId;
    }


    public void setOrderId(String orderId) {
    this.orderId = orderId;
    }


    public Double getPrice() {
    return price;
    }


    public void setPrice(Double price) {
    this.price = price;
    }


    @Override
    public String toString() {
    return orderId + "\t" + price;
    }
    }


    package com.lagou.mr.group;


    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.mapreduce.Partitioner;


    public class CustomPartitioner extends Partitioner<OrderBean, NullWritable>
    {
    @Override
    public int getPartition(OrderBean orderBean, NullWritable nullWritable,
    int i) {
    //自定义分区，将相同订单id的数据发送到同一个reduce里面去
    return (orderBean.getOrderId().hashCode() & Integer.MAX_VALUE) % i;
    }
    }


    package com.lagou.mr.group;


    import org.apache.hadoop.io.WritableComparable;
    import org.apache.hadoop.io.WritableComparator;


    Mapper


    Reducer


    public class CustomGroupingComparator extends WritableComparator {
    //将我们自定义的OrderBean注册到我们自定义的CustomGroupIngCompactor当中来
    //表示我们的分组器在分组的时候，对OrderBean这一种类型的数据进行分组
    //传入作为key的bean的class类型，以及制定需要让框架做反射获取实例对象
    public CustomGroupingComparator() {
    super(OrderBean.class, true);
    }


    @Override
    public int compare(WritableComparable a, WritableComparable b) {
    OrderBean first = (OrderBean) a;
    OrderBean second = (OrderBean) b;
    final int i = first.getOrderId().compareTo(second.getOrderId());
    if (i == 0 ) {
    System.out.println(first.getOrderId() + "----" +
    second.getOrderId());
    }
    return i;
    }
    }


    package com.lagou.mr.group;


    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;


    import java.io.IOException;


    public class GroupMapper extends Mapper<LongWritable, Text, OrderBean,
    NullWritable> {


    @Override
    protected void map(LongWritable key, Text value, Context context) throws
    IOException, InterruptedException {
    //读取一行文本，然后切分
    final String[] arr = value.toString().split("\t");
    final OrderBean bean = new OrderBean(arr[ 0 ],
    Double.parseDouble(arr[ 2 ]));
    context.write(bean, NullWritable.get());
    }
    }


package com.lagou.mr.group;

    Driver


import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.mapreduce.Reducer;

import java.io.IOException;

public class GroupReducer extends Reducer<OrderBean, NullWritable, OrderBean,
NullWritable> {
@Override
protected void reduce(OrderBean key, Iterable<NullWritable> values, Context
context) throws IOException, InterruptedException {
System.out.println();
context.write(key, NullWritable.get());
}
}

    package com.lagou.mr.group;


    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;


    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


    import java.io.IOException;
    public class GroupDriver {
    public static void main(String[] args) throws IOException,
    ClassNotFoundException, InterruptedException {


    // 1 获取配置信息，或者job对象实例
    Configuration configuration = new Configuration();
    Job job = Job.getInstance(configuration);


    // 2 指定本程序的jar包所在的本地路径
    job.setJarByClass(GroupDriver.class);


    // 3 指定本业务job要使用的mapper/Reducer业务类
    job.setMapperClass(GroupMapper.class);
    job.setReducerClass(GroupReducer.class);


    // 4 指定mapper输出数据的kv类型
    job.setMapOutputKeyClass(OrderBean.class);
    job.setMapOutputValueClass(NullWritable.class);


    // 5 指定最终输出的数据的kv类型
    job.setOutputKeyClass(OrderBean.class);
    job.setOutputValueClass(NullWritable.class);


    // 6 指定job的输入原始文件所在目录


    userId positionId date
    
    1001 177725422 2020-01-03
    
    1002 177725422 2020-01-04
    
    1002 177725433 2020-01-03
    
    id positionName
    
    177725422 产品经理
    
    177725433 大数据开发工程师

5.7 MapReduce Join实战

5.7.1 MR reduce端join

1.1 需求分析

需求：

投递行为数据表deliver_info：

职位表position

假如数据量巨大，两表的数据是以文件的形式存储在HDFS中，需要用mapreduce程序来实现一下SQL
查询运算

1.2 代码实现

通过将关联的条件作为map输出的key，将两表满足join条件的数据并携带数据所来源的文件信息，发
往同一个reduce task，在reduce中进行数据的串联

Driver

    FileInputFormat.setInputPaths(job, new Path("E:\\teach\\hadoop框架
    \\资料\\data\\GroupingComparator"));
    FileOutputFormat.setOutputPath(job, new Path("E:\\group-out"));
    // 7 指定分区器，指定分组比较器，设置reducetask数量
    job.setPartitionerClass(CustomPartitioner.class);
    job.setGroupingComparatorClass(CustomGroupingComparator.class);
    job.setNumReduceTasks( 2 );
    // 8 将job中配置的相关参数，以及job所用的java类所在的jar包， 提交给yarn去运行
    boolean result = job.waitForCompletion(true);
    System.exit(result? 0 : 1 );
    
    boolean b = job.waitForCompletion(true);
    System.exit(b? 0 : 1 );
    }
    }


    package com.lagou.mr.join.reduce_join;


Mapper

    import com.lagou.mr.wc.WordCountCombiner;
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


    import java.io.IOException;


    public class ReduceJoinDriver {
    public static void main(String[] args) throws IOException,
    ClassNotFoundException, InterruptedException {


    // 1. 获取配置文件对象，获取job对象实例
    final Configuration conf = new Configuration();
    final Job job = Job.getInstance(conf, "ReduceJoinDriver");
    // 2. 指定程序jar的本地路径
    job.setJarByClass(ReduceJoinDriver.class);
    // 3. 指定Mapper/Reducer类
    job.setMapperClass(ReduceJoinMapper.class);
    job.setReducerClass(ReduceJoinReducer.class);
    // 4. 指定Mapper输出的kv数据类型
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(DeliverBean.class);
    // 5. 指定最终输出的kv数据类型
    job.setOutputKeyClass(DeliverBean.class);
    job.setOutputValueClass(NullWritable.class);
    // 6. 指定job输出结果路径
    FileInputFormat.setInputPaths(job, new Path(args[ 0 ])); //指定读取数据的原始
    路径
    // 7. 指定job输出结果路径
    FileOutputFormat.setOutputPath(job, new Path(args[ 1 ])); //指定结果数据输出
    路径
    // 8. 提交作业
    final boolean flag = job.waitForCompletion(true);
    //jvm退出：正常退出 0 ，非 0 值则是错误退出
    System.exit(flag? 0 : 1 );


    }
    }


    package com.lagou.mr.join.reduce_join;


    import com.sun.org.apache.bcel.internal.generic.NEW;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;
    import org.apache.hadoop.mapreduce.lib.input.FileSplit;


Reducer

    import java.io.IOException;


    public class ReduceJoinMapper extends Mapper<LongWritable, Text, Text,
    DeliverBean> {


    String name;
    DeliverBean bean = new DeliverBean();
    Text k = new Text();


    @Override
    protected void setup(Context context) throws IOException,
    InterruptedException {
    // 1 获取输入文件切片
    FileSplit split = (FileSplit) context.getInputSplit();
    // 2 获取输入文件名称
    name = split.getPath().getName();
    }


    @Override
    protected void map(LongWritable key, Text value, Context context) throws
    IOException, InterruptedException {


    // 1 获取输入数据
    String line = value.toString();
    // 2 不同文件分别处理
    if (name.startsWith("deliver_info")) {
    // 2.1 切割
    String[] fields = line.split("\t");
    // 2.2 封装bean对象
    bean.setUserId(fields[ 0 ]);
    bean.setPositionId(fields[ 1 ]);
    bean.setDate(fields[ 2 ]);
    bean.setPositionName("");
    bean.setFlag("deliver");
    k.set(fields[ 1 ]);
    } else {
    // 2.3 切割
    String[] fields = line.split("\t");
    // 2.4 封装bean对象
    bean.setPositionId(fields[ 0 ]);
    bean.setPositionName(fields[ 1 ]);
    bean.setUserId("");
    bean.setDate("");
    bean.setFlag("position");
    k.set(fields[ 0 ]);
    }
    // 3 写出
    context.write(k, bean);
    }
    }


    package com.lagou.mr.join.reduce_join;


Bean

    import org.apache.commons.beanutils.BeanUtils;
    import org.apache.hadoop.io.IntWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Reducer;


    import java.io.IOException;
    import java.util.ArrayList;


    public class ReduceJoinReducer extends Reducer<Text, DeliverBean, DeliverBean,
    NullWritable> {


    @Override
    protected void reduce(Text key, Iterable<DeliverBean> values, Context
    context) throws IOException, InterruptedException {
    // 1准备投递行为数据的集合
    ArrayList<DeliverBean> deBeans = new ArrayList<>();
    // 2 准备bean对象
    DeliverBean pBean = new DeliverBean();
    for (DeliverBean bean : values) {
    if ("deliver".equals(bean.getFlag())) {//
    DeliverBean dBean = new DeliverBean();
    try {
    BeanUtils.copyProperties(dBean, bean);
    } catch (Exception e) {
    e.printStackTrace();
    }
    deBeans.add(dBean);
    } else {
    try {
    BeanUtils.copyProperties(pBean, bean);
    } catch (Exception e) {
    e.printStackTrace();
    }
    }
    }
    // 3 表的拼接
    for (DeliverBean bean : deBeans) {
    bean.setPositionName(pBean.getPositionName());
    // 4 数据写出去
    context.write(bean, NullWritable.get());
    }
    }
    }


    package com.lagou.mr.join.reduce_join;


    import org.apache.hadoop.io.Writable;


    import java.io.DataInput;
    import java.io.DataOutput;
    import java.io.IOException;


public class DeliverBean implements Writable {
private String userId;
private String positionId;
private String date;
private String positionName;

private String flag;
public DeliverBean() {
}

public DeliverBean(String userId, String positionId, String date, String
positionName,
String flag) {
this.userId = userId;
this.positionId = positionId;
this.date = date;
this.positionName = positionName;

this.flag = flag;
}

public String getUserId() {
return userId;
}

public void setUserId(String userId) {
this.userId = userId;
}

public String getPositionId() {
return positionId;
}

public void setPositionId(String positionId) {
this.positionId = positionId;
}

public String getDate() {
return date;
}

public void setDate(String date) {
this.date = date;
}

public String getPositionName() {
return positionName;
}

public void setPositionName(String positionName) {
this.positionName = positionName;
}

public String getFlag() {
return flag;

缺点：这种方式中，join的操作是在reduce阶段完成，reduce端的处理压力太大，map节点的运算负载
则很低，资源利用率不高，且在reduce阶段极易产生数据倾斜

解决方案： map端join实现方式

5.7.2 MR map端join

2.1 需求分析

适用于关联表中有小表的情形；

可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行join并
输出最终结果，可以大大提高join操作的并发度，加快处理速度

2.2 代码实现

    在Mapper的setup阶段，将文件读取到缓存集合中
    在驱动函数中加载缓存。

// 缓存普通文件到Task运行节点。

job.addCacheFile(new URI("file:///e:/cache/position.txt"));

}

    public void setFlag(String flag) {
    this.flag = flag;
    }
    
    @Override
    public void write(DataOutput out) throws IOException {
    out.writeUTF(userId);
    out.writeUTF(positionId);
    out.writeUTF(date);
    out.writeUTF(positionName);
    out.writeUTF(flag);
    }
    
    @Override
    public void readFields(DataInput in) throws IOException {
    this.userId = in.readUTF();
    this.positionId = in.readUTF();
    this.date = in.readUTF();
    this.positionName = in.readUTF();
    this.flag=in.readUTF();
    }
    
    @Override
    public String toString() {
    return "DeliverBean{" +
    "userId='" + userId + '\'' +
    ", positionId='" + positionId + '\'' +
    ", date='" + date + '\'' +
    ", positionName='" + positionName ;
    }
    }


Driver

Mapper

    package com.lagou.mr.join.map_join;


    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


    import java.io.IOException;
    import java.net.URI;
    import java.net.URISyntaxException;


    public class MapJoinDriver {
    public static void main(String[] args) throws IOException,
    ClassNotFoundException, InterruptedException, URISyntaxException {


    // 1. 获取配置文件对象，获取job对象实例
    final Configuration conf = new Configuration();
    final Job job = Job.getInstance(conf, "ReduceJoinDriver");
    // 2. 指定程序jar的本地路径
    job.setJarByClass(MapJoinDriver.class);
    // 3. 指定Mapper类
    job.setMapperClass(MapJoinMapper.class);
    // 4. 指定最终输出的kv数据类型
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(NullWritable.class);
    //5.指定job读取数据路径
    FileInputFormat.setInputPaths(job, new Path(args[ 0 ])); //指定读取数据的原始
    路径
    // 6. 指定job输出结果路径
    FileOutputFormat.setOutputPath(job, new Path(args[ 1 ])); //指定结果数据输出
    路径
    // 7.加载缓存文件
    job.addCacheFile(new URI("file:///E:/map_join/cache/position.txt"));
    job.setNumReduceTasks( 0 );
    // 8. 提交作业
    final boolean flag = job.waitForCompletion(true);
    //jvm退出：正常退出 0 ，非 0 值则是错误退出
    System.exit(flag? 0 : 1 );


    }
    }


    package com.lagou.mr.join.map_join;


    import org.apache.commons.lang3.StringUtils;
    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;


import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;

import java.io.BufferedReader;
import java.io.FileInputStream;
import java.io.IOException;
import java.io.InputStreamReader;
import java.util.HashMap;
import java.util.Map;

public class MapJoinMapper extends Mapper<LongWritable, Text, Text,
NullWritable> {

String name;
DeliverBean bean = new DeliverBean();
Text k = new Text();
Map<String, String> pMap = new HashMap<>();

//读取文件
@Override
protected void setup(Context context) throws IOException,
InterruptedException {
// 1 获取缓存的文件
BufferedReader reader = new BufferedReader(new InputStreamReader(new
FileInputStream("position.txt"),"UTF-8"));

String line;
while(StringUtils.isNotEmpty(line = reader.readLine())){
// 2 切割
String[] fields = line.split("\t");
// 3 缓存数据到集合
pMap.put(fields[0], fields[1]);
}
// 4 关流
reader.close();

}

@Override
protected void map(LongWritable key, Text value, Context context) throws
IOException, InterruptedException {

// 1 获取一行
String line = value.toString();
// 2 截取
String[] fields = line.split("\t");
// 3 获取职位id
String pId = fields[1];
// 4 获取职位名称
String pName = pMap.get(pId);
// 5 拼接
k.set(line + "\t"+ pName);
// 写出
context.write(k, NullWritable.get());
}

DeliverBean

}

    package com.lagou.mr.join.map_join;


    import org.apache.hadoop.io.Writable;


    import java.io.DataInput;
    import java.io.DataOutput;
    import java.io.IOException;


    public class DeliverBean implements Writable {
    private String userId;
    private String positionId;
    private String date;
    private String positionName;


    private String flag;
    public DeliverBean() {
    }


    public DeliverBean(String userId, String positionId, String date, String
    positionName, String flag) {
    this.userId = userId;
    this.positionId = positionId;
    this.date = date;
    this.positionName = positionName;


    this.flag = flag;
    }


    public String getUserId() {
    return userId;
    }


    public void setUserId(String userId) {
    this.userId = userId;
    }


    public String getPositionId() {
    return positionId;
    }


    public void setPositionId(String positionId) {
    this.positionId = positionId;
    }


    public String getDate() {
    return date;
    }


    public void setDate(String date) {
    this.date = date;
    }


5.7.3 数据倾斜解决方案

什么是数据倾斜？

    数据倾斜无非就是大量的相同key被partition分配到一个分区里,


现象

    绝大多数task执行得都非常快，但个别task执行的极慢。甚至失败！


通用解决方案：

    public String getPositionName() {
    return positionName;
    }


    public void setPositionName(String positionName) {
    this.positionName = positionName;
    }


    public String getFlag() {
    return flag;
    }


    public void setFlag(String flag) {
    this.flag = flag;
    }


    @Override
    public void write(DataOutput out) throws IOException {
    out.writeUTF(userId);
    out.writeUTF(positionId);
    out.writeUTF(date);
    out.writeUTF(positionName);
    out.writeUTF(flag);
    }


    @Override
    public void readFields(DataInput in) throws IOException {
    this.userId = in.readUTF();
    this.positionId = in.readUTF();
    this.date = in.readUTF();
    this.positionName = in.readUTF();
    this.flag=in.readUTF();
    }


    @Override
    public String toString() {
    return "DeliverBean{" +
    "userId='" + userId + '\'' +
    ", positionId='" + positionId + '\'' +
    ", date='" + date + '\'' +
    ", positionName='" + positionName + '\'' +
    '}';
    }
    }


对key增加随机数。

5.8 MapReduce读取和输出数据

5.8.1 InputFormat

运行MapReduce程序时，输入的文件格式包括:基于行的日志文件、二进制格式文件、数据库表等。那
么，针对不同的数据类型，MapReduce是如何读取这些数据的呢?

InputFormat是MapReduce框架用来读取数据的类。

InputFormat常见子类包括:

    TextInputFormat （普通文本文件，MR框架默认的读取实现类型）
    KeyValueTextInputFormat（读取一行文本数据按照指定分隔符，把数据封装为kv类型）
    NLineInputF ormat(读取数据按照行数进行划分分片)
    CombineTextInputFormat(合并小文件，避免启动过多MapTask任务)
    自定义InputFormat


    1. CombineTextInputFormat案例
    MR框架默认的 TextInputFormat 切片机制按文件划分切片，文件无论多小，都是单独一个切片，
    然后由一个MapTask处理，如果有大量小文件，就对应的会生成并启动大量的 MapTask，而每个
    MapTask处理的数据量很小大量时间浪费在初始化资源启动收回等阶段，这种方式导致资源利用
    率不高。
    CombineTextInputForma t用于小文件过多的场景，它可以将 多个小文件从逻辑上划分成一个切
    片 ，这样多个小文件就可以交给一个MapTask处理，提高资源利用率。
    需求
    将输入数据中的多个小文件合并为一个切片处理
    运行WordCount案例，准备 多个小文件


具体使用方式

验证切片数量的变化！！

    CombineTextInputFormat切片原理
    切片生成过程分为两部分：虚拟存储过程和切片过程
    假设设置setMaxInputSplitSize值为4M
    四个小文件：1.txt -->2M ;2.txt-->7M;3.txt-->0.3M;4.txt--->8.2M
    虚拟存储过程：把输入目录下所有文件大小，依次和设置的setMaxInputSplitSize值进行比
    较，如果不大于设置的最大值，逻辑上划分一个 块 。如果输入文件大于设置的最大值且大于
    两倍，那么以最大值切割一块；当剩余数据大小超过设置的最大值且不大于最大值 2 倍，此时
    将文件均分成 2 个虚拟存储块（防止出现太小切片）。
    比如如setMaxInputSplitSize值为4M，输入文件大小为8.02M，则先逻辑上分出一个4M的
    块。剩余的大小为4.02M，如果按照4M逻辑划分，就会出现0.02M的非常小的虚拟存储文
    件，所以将剩余的4.02M文件切分成（2.01M和2.01M）两个文件。
    1.txt-->2M;2M<4M;一个块；


    // 如果不设置InputFormat，它默认用的是TextInputFormat.class
    job.setInputFormatClass(CombineTextInputFormat.class);


    //虚拟存储切片最大值设置4m
    CombineTextInputFormat.setMaxInputSplitSize(job, 4194304 );


    2.txt-->7M;7M>4M,但是不大于两倍，均匀分成两块；两块：每块3.5M；
    3.txt-->0.3M;0.3<4M ,0.3M<4M ,一个块
    4.txt-->8.2M;大于最大值且大于两倍；一个4M的块，剩余4.2M分成两块，每块2.1M
    所有块信息：
    2M，3.5M，3.5M，0.3M，4M，2.1M，2.1M 共 7 个虚拟存储块。
    切片过程
    判断虚拟存储的文件大小是否大于setMaxInputSplitSize值，大于等于则单独形成一个
    切片。
    如果不大于则跟下一个虚拟存储文件进行合并，共同形成一个切片。
    按照之前输入文件：有 4 个小文件大小分别为2M、7M、0.3M以及8.2M这四个小文件，
    则虚拟存储之后形成 7 个文件块，大小分别为：
    2M，3.5M，3.5M，0.3M，4M，2.1M，2.1M
    最终会形成 3 个切片，大小分别为：
    （2+3.5）M，（3.5+0.3+4）M，（2.1+2.1）M


注意：虚拟存储切片最大值设置最好根据实际的小文件大小情况来设置具体的值。

    2. 自定义InputFormat
    HDFS还是MapReduce，在处理小文件时效率都非常低，但又难免面临处理大量小文件的场景，
    此时，就需要有相应解决方案。可以自定义InputFormat实现小文件的合并。
    需求
    将多个小文件合并成一个SequenceFile文件（SequenceFile文件是Hadoop用来存储二进制形式的
    key-value 对的文件格式），SequenceFile里面存储着多个文件，存储的形式为文件路径+名称为
    key，文件内容为value。
    结果
    得到一个合并了多个小文件的SequenceFile文件
    整体思路
    1. 定义一个类继承FileInputFormat
    2. 重写isSplitable()指定为不可切分；重写createRecordReader()方法，创建自己的
    RecorderReader对象
    3. 改变默认读取数据方式，实现一次读取一个完整文件作为kv输出；
    4. Driver指定使用的InputFormat类型


代码参考

自定义InputFormat

    package com.lagou.mr.inputformat;


    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.BytesWritable;


自定义RecordReader

    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.InputSplit;
    import org.apache.hadoop.mapreduce.JobContext;
    import org.apache.hadoop.mapreduce.RecordReader;
    import org.apache.hadoop.mapreduce.TaskAttemptContext;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;


    import java.io.IOException;


    public class CustomFileInputformat extends FileInputFormat<Text, BytesWritable>
    {


    //文件不可切分
    @Override
    protected boolean isSplitable(JobContext context, Path filename) {
    return false;
    }


    //获取自定义RecordReader对象用来读取数据
    @Override
    public RecordReader<Text, BytesWritable> createRecordReader(InputSplit
    split, TaskAttemptContext context)
    throws IOException, InterruptedException {


    CustomRecordReader recordReader = new CustomRecordReader();
    recordReader.initialize(split, context);


    return recordReader;
    }
    }


    package com.lagou.mr.inputformat;


    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.FSDataInputStream;
    import org.apache.hadoop.fs.FileSystem;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.BytesWritable;
    import org.apache.hadoop.io.IOUtils;
    import org.apache.hadoop.io.Text;


    import org.apache.hadoop.mapreduce.InputSplit;
    import org.apache.hadoop.mapreduce.RecordReader;
    import org.apache.hadoop.mapreduce.TaskAttemptContext;
    import org.apache.hadoop.mapreduce.lib.input.FileSplit;


    import java.io.IOException;


    public class CustomRecordReader extends RecordReader<Text, BytesWritable> {
    private Configuration configuration;


    //切片


private FileSplit split;

//是否读取到内容的标识符
private boolean isProgress = true;
//输出的kv
private BytesWritable value = new BytesWritable();
private Text k = new Text();

@Override
public void initialize(InputSplit split, TaskAttemptContext context) throws
IOException, InterruptedException {
//获取到文件切片以及配置文件对象
this.split = (FileSplit) split;
configuration = context.getConfiguration();
}

@Override
public boolean nextKeyValue() throws IOException, InterruptedException {
if (isProgress) {

// 1 定义缓存区
byte[] contents = new byte[(int) split.getLength()];

FileSystem fs = null;
FSDataInputStream fis = null;

try {
// 2 获取文件系统
Path path = split.getPath();
fs = path.getFileSystem(configuration);

// 3 读取数据
fis = fs.open(path);

// 4 读取文件内容
IOUtils.readFully(fis, contents, 0 , contents.length);

// 5 输出文件内容
value.set(contents, 0 , contents.length);

// 6 获取文件路径及名称
String name = split.getPath().toString();

// 7 设置输出的key值
k.set(name);

} catch (Exception e) {

} finally {
IOUtils.closeStream(fis);
}

isProgress = false;

return true;
}

return false;

Mapper

Reducer

}

    @Override
    public Text getCurrentKey() throws IOException, InterruptedException {
    //返回key
    return k;
    }


    @Override
    public BytesWritable getCurrentValue() throws IOException,
    InterruptedException {
    //返回value
    return value;
    }


    @Override
    public float getProgress() throws IOException, InterruptedException {
    return 0 ;
    }


    @Override
    public void close() throws IOException {


    }
    }


    package com.lagou.mr.inputformat;


    import org.apache.hadoop.io.BytesWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;


    import java.io.IOException;


    public class SequenceFileMapper extends Mapper<Text,
    BytesWritable,Text,BytesWritable> {
    @Override
    protected void map(Text key, BytesWritable value, Context context) throws
    IOException, InterruptedException {
    //读取内容直接输出
    context.write(key, value);
    }
    }


    package com.lagou.mr.inputformat;


    import org.apache.hadoop.io.BytesWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Reducer;


Driver

    import java.io.IOException;


    public class SequenceFileReducer extends Reducer<Text,
    BytesWritable,Text,BytesWritable> {
    @Override
    protected void reduce(Text key, Iterable<BytesWritable> values, Context
    context) throws IOException, InterruptedException {
    //输出value值，其中只有一个BytesWritable 所以直接next取出即可
    context.write(key, values.iterator().next());
    }
    }


    package com.lagou.mr.inputformat;


    import com.lagou.mr.partition.*;
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.BytesWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;


    import java.io.IOException;


    public class SequenceFileDriver {


    public static void main(String[] args) throws IOException,
    ClassNotFoundException, InterruptedException {
    final Configuration conf = new Configuration();
    final Job job = Job.getInstance(conf);


    job.setJarByClass(SequenceFileDriver.class);


    job.setMapperClass(SequenceFileMapper.class);
    job.setReducerClass(SequenceFileReducer.class);


    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(BytesWritable.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(BytesWritable.class);


    job.setInputFormatClass(CustomFileInputformat.class);
    job.setOutputFormatClass(SequenceFileOutputFormat.class);


    FileInputFormat.setInputPaths(job, new Path(args[ 0 ]));
    FileOutputFormat.setOutputPath(job, new Path(args[ 1 ]));


    final boolean flag = job.waitForCompletion(true);
    System.exit(flag? 0 : 1 );
    }
    }


验证输出结果

5.8.2 OutputFormat

OutputFormat:是MapReduce输出数据的基类，所有MapReduce的数据输出都实现了OutputFormat
抽象类。下面我们介绍几种常见的OutputFormat子类

    TextOutputFormat
    默认的输出格式是TextOutputFormat，它把每条记录写为文本行。它的键和值可以是任意类型，
    因为TextOutputFormat调用toString()方 法把它们转换为字符串。
    SequenceFileOutputFormat
    将SequenceFileOutputFormat输出作为后续MapReduce任务的输入，这是一种好的输出格式，
    因为它的格式紧凑，很容易被压缩。


自定义OutputFormat

需求分析

要在一个MapReduce程序中根据数据的不同输出两类结果到不同 目录 ，这类输出需求可以通过自定义
OutputFormat来实现。

实现步骤

    1. 自定义一个类继承FileOutputFormat。
    2. 改写RecordWriter，改写输出数据的方法write()。


需求

网络请求日志数据

输出结果

lagou.log

other.log

    http://www.baidu.com
    http://www.google.com
    http://cn.bing.com
    http://www.lagou.com
    http://www.sohu.com
    http://www.sina.com
    http://www.sin2a.com
    http://www.sin2desa.com
    http://www.sindsafa.com


    http://www.lagou.com


参考代码

Mapper

Reducer

OutputFormat

    http://cn.bing.com
    http://www.baidu.com
    http://www.google.com
    http://www.sin2a.com
    http://www.sin2desa.com
    http://www.sina.com
    http://www.sindsafa.com
    http://www.sohu.com


    package com.lagou.mr.outputformat;


    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;


    import java.io.IOException;


    public class OutputMapper extends Mapper<LongWritable, Text,Text, NullWritable>
    {
    @Override
    protected void map(LongWritable key, Text value, Context context) throws
    IOException, InterruptedException {
    context.write(value, NullWritable.get());
    }
    }


    package com.lagou.mr.outputformat;


    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Reducer;


    import java.io.IOException;


    public class OutputReducer extends Reducer<Text, NullWritable,Text,NullWritable>
    {
    @Override
    protected void reduce(Text key, Iterable<NullWritable> values, Context
    context) throws IOException, InterruptedException {
    context.write(key,NullWritable.get() );
    }
    }


RecordWriter

    package com.lagou.mr.outputformat;


    import org.apache.hadoop.fs.FSDataOutputStream;
    import org.apache.hadoop.fs.FileSystem;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.RecordWriter;
    import org.apache.hadoop.mapreduce.TaskAttemptContext;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


    import java.io.IOException;


    public class CustomOutputFormat extends FileOutputFormat<Text, NullWritable> {
    @Override
    public RecordWriter<Text, NullWritable> getRecordWriter(TaskAttemptContext
    context)
    throws IOException, InterruptedException {
    
    //获取文件系统对象
    final FileSystem fs = FileSystem.get(context.getConfiguration());
    //指定输出数据的文件
    final Path lagouPath = new Path("e:/lagou.log");
    final Path otherLog = new Path("e:/other.log");
    //获取输出流
    final FSDataOutputStream lagouOut = fs.create(lagouPath);
    final FSDataOutputStream otherOut = fs.create(otherLog);
    return new CustomWriter(lagouOut, otherOut);
    }
    }
    
    package com.lagou.mr.outputformat;
    
    import org.apache.hadoop.fs.FSDataOutputStream;
    import org.apache.hadoop.io.IOUtils;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.RecordWriter;
    import org.apache.hadoop.mapreduce.TaskAttemptContext;
    
    import java.io.IOException;
    
    public class CustomWriter extends RecordWriter<Text, NullWritable> {
    private FSDataOutputStream lagouOut;
    private FSDataOutputStream otherOut;
    
    public CustomWriter(FSDataOutputStream lagouOut,FSDataOutputStream otherOut)
    {
    this.lagouOut=lagouOut;
    this.otherOut=otherOut;
    }

Driver

    @Override
    public void write(Text key, NullWritable value) throws IOException,
    InterruptedException {
    // 判断是否包含“lagou”输出到不同文件
    if (key.toString().contains("lagou")) {
    lagouOut.write(key.toString().getBytes());
    lagouOut.write("\r\n".getBytes());
    } else {
    otherOut.write(key.toString().getBytes());
    otherOut.write("\r\n".getBytes());
    }
    }
    
    @Override
    public void close(TaskAttemptContext context) throws IOException,
    InterruptedException {
    
    IOUtils.closeStream(lagouOut);
    IOUtils.closeStream(otherOut);
    }
    }
    
    package com.lagou.mr.outputformat;
    
    import com.lagou.mr.partition.*;
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    
    import java.io.IOException;
    
    public class OutputDriver {
    public static void main(String[] args) throws IOException,
    ClassNotFoundException, InterruptedException {
    final Configuration conf = new Configuration();
    final Job job = Job.getInstance(conf);
    
    job.setJarByClass(OutputDriver.class);
    
    job.setMapperClass(OutputMapper.class);
    job.setReducerClass(OutputReducer.class);
    
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(NullWritable.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(NullWritable.class);
    
    job.setOutputFormatClass(CustomOutputFormat.class);


压缩格式

    hadoop
    自带


算法

文件扩

展名

是否可

切分

换成压缩格式后，原来的程序

是否需要修改

DEFLATE

是，直接

使用

    DEFLATE .deflate 否 和文本处理一样，不需要修改


    Gzip


是，直接

使用

    DEFLATE .gz 否 和文本处理一样，不需要修改


    bzip2


是，直接

使用

    bzip2 .bz2 是 和文本处理一样，不需要修改


LZO

否，需要

安装

    LZO .lzo 是


需要建索引，还需要指定输入

格式

    Snappy


否，需要

安装

    Snappy .snappy 否 和文本处理一样，不需要修改


验证结果是否已把数据分别输出到不同的目录中！！

5.9 shuffle阶段数据的压缩机制

5.9.1 hadoop当中支持的压缩算法

数据压缩有两大好处，节约磁盘空间，加速数据在网络和磁盘上的传输！！

我们可以使用bin/hadoop checknative 来查看我们编译之后的hadoop支持的各种压缩，如果出现
openssl为false，那么就在线安装一下依赖包！！

安装openssl

    FileInputFormat.setInputPaths(job, new Path("e:/click_log.data"));


    // outputformat继承自fileoutputformat 而fileoutputformat要输出一个_SUCCESS
    文件，所以，在这还得指定一个输出目录
    FileOutputFormat.setOutputPath(job, new Path("e:/click_log/output"));


    final boolean flag = job.waitForCompletion(true);
    System.exit(flag? 0 : 1 );
    }
    }


    yum install -y openssl-devel


压缩格式 对应的编码/解码器

    DEFLATE org.apache.hadoop.io.compress.DefaultCodec


    gzip org.apache.hadoop.io.compress.GzipCodec


    bzip2 org.apache.hadoop.io.compress.BZip2Codec


    LZO com.hadoop.compression.lzo.LzopCodec


    Snappy org.apache.hadoop.io.compress.SnappyCodec


压缩算法 原始文件大小 压缩后的文件大小 压缩速度 解压缩速度

    gzip 8.3GB 1.8GB 17.5MB/s 58MB/s


    bzip2 8.3GB 1.1GB 2.4MB/s 9.5MB/s


    LZO-bset 8.3GB 2GB 4MB/s 60.6MB/s


    LZO 8.3GB 2.9GB 49.3MB/S 74.6MB/s


为了支持多种压缩/解压缩算法，Hadoop引入了编码/解码器

常见压缩方式对比分析

5.9.2 压缩位置

    Map输入端压缩
    此处使用压缩文件作为Map的输入数据，无需显示指定编解码方式，Hadoop会自动检查文件扩展
    名，如果压缩方式能够匹配，Hadoop就会选择合适的编解码方式对文件进行压缩和解压。
    Map输出端压缩
    Shuffle是Hadoop MR过程中资源消耗最多的阶段，如果有数据量过大造成网络传输速度缓慢，可
    以考虑使用压缩
    Reduce端输出压缩
    输出的结果数据使用压缩能够减少存储的数据量，降低所需磁盘的空间，并且作为第二个MR的输
    入时可以复用压缩。


5.9.3 压缩配置方式

    1. 在驱动代码中通过Configuration直接设置使用的压缩方式，可以开启Map输出和Reduce输出压缩


    2. 配置mapred-site.xml(修改后分发到集群其它节点，重启Hadoop集群),此种方式对运行在集群的
    所有MR任务都会执行压缩。


5.9.4 压缩案例

需求

使用snappy压缩方式压缩WordCount案例的输出结果数据

具体实现

在驱动代码中添加压缩配置

重新打成jar包，提交集群运行，验证输出结果是否已进行了snappy压缩！！

第 六 节 MR综合案例

    设置map阶段压缩
    Configuration configuration = new Configuration();
    configuration.set("mapreduce.map.output.compress","true");
    configuration.set("mapreduce.map.output.compress.codec","org.apache.hadoop.i
    o.compress.SnappyCodec");
    设置reduce阶段的压缩
    configuration.set("mapreduce.output.fileoutputformat.compress","true");
    configuration.set("mapreduce.output.fileoutputformat.compress.type","RECORD"
    );
    configuration.set("mapreduce.output.fileoutputformat.compress.codec","org.ap
    ache.hadoop.io.compress.SnappyCodec");
    
    <property>
    <name>mapreduce.output.fileoutputformat.compress</name>
    <value>true</value>
    </property>
    <property>
    <name>mapreduce.output.fileoutputformat.compress.type</name>
    <value>RECORD</value>
    </property>
    <property>
    <name>mapreduce.output.fileoutputformat.compress.codec</name>
    <value>org.apache.hadoop.io.compress.SnappyCodec</value>
    </property>
    
    configuration.set("mapreduce.output.fileoutputformat.compress","true");
    configuration.set("mapreduce.output.fileoutputformat.compress.type","RECORD");
    configuration.set("mapreduce.output.fileoutputformat.compress.codec","org.apache
    .hadoop.io.compress.SnappyCodec");

1.1 需求

现在有一些订单的评论数据，需求，将订单按照好评与差评区分开来，将数据输出到不同的文件目录

下，数据内容如下，其中数据第九个字段表示好评，中评，差评。 0 ：好评， 1 ：中评， 2 ：差评。

现需要根据好评，中评，差评把数据分类并输出到不同的目录中,并且要求按照时间顺序降序排列。

备注：

现在有大量类似上面的小文件!

1.2 分析

    自定义InputFormat合并小文件
    自定义分区根据评论等级把数据分区
    自定义OutputFormat把数据输出到多个目录

1.3 开发步骤

1 、合并小文件

1 、Mapper

300 东西很不错，物流也很快 \N 1 106 131**33 0 2019-02-06 19:10:13

301 还行，洗完有点干，不知道怎么回事 \N 1 106 136**44 0 2019-03-22

14:16:41

302 还可以吧，保质期短，感觉貌似更天然些 \N 1 106 134**34 0 2019-04-10

13:40:06

303 还可以吧，保质期短，感觉貌似更天然些 \N 1 105 134**33 0 2019-01-15

14:40:21

304 还没用，，不知道效果怎么样 \N 1 105 137**66 0 2019-02-28 18:55:43

305 刚收到，还没用，用后再追评！不过，听朋友说好用，才买的！ \N 1 105 138**60

0 2019-03-13 19:10:09

306 一般，感觉用着不是很好，可能我头发太干了 \N 1 105 132**44 0 2019-

04-09 10:35:49

307 非常好用，之前买了 10 支，这次又买了 10 支，不错，会继续支持！ \N 1 103 131**33

0 2019-01-15 13:10:46

308 喜欢茶树油的 \N 1 103 135**33 0 2019-02-08 14:35:09

309 好像比其他的强一些，继续使用中 \N 1 103 133**99 0 2019-03-14

19:55:36

310 感觉洗后头发很干净，头皮有一定改善。 \N 1 103 138**44 0 2019-04-09

22:55:59

311 从出生到现在一直都是惠氏 现在宝宝两周半了 \N 1 157 那***情 0 2017-12-

01 06:05:30

    312 口感不错，孩子很喜欢。推荐。 \N 1 157 w***4 0 2017-12-12 08:35:06
    313 价格优惠，日期新鲜，包装完好！发货速度快，非常喜欢！还有赠品！ \N 1 157 j***0
    0 2019-01-09 22:55:41


    package com.lagou.mr.comment.step1;


    import org.apache.hadoop.io.BytesWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;


    import java.io.IOException;


    //text：代表的是一个文件的path+名称，BytesWritable：一个文件的内容


2 、自定义InputFormat

MergeInputFormat

MergeRecordReader

    public class MergeMapper extends Mapper<Text, BytesWritable, Text,
    BytesWritable> {
    @Override
    protected void map(Text key, BytesWritable value, Context context) throws
    IOException, InterruptedException {
    context.write(key, value);
    }
    }


    package com.lagou.mr.comment.step1;
    //自定义inputformat读取多个小文件合并为一个SequenceFile文件


    //SequenceFile文件中以kv形式存储文件，key--》文件路径+文件名称，value-->文件的整个内容


    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.BytesWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.InputSplit;
    import org.apache.hadoop.mapreduce.JobContext;
    import org.apache.hadoop.mapreduce.RecordReader;
    import org.apache.hadoop.mapreduce.TaskAttemptContext;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;


    import java.io.IOException;


    //TextInputFormat中泛型是LongWritable：文本的偏移量, Text：一行文本内容；指明当前
    inputformat的输出数据类型
    //自定义inputformat：key-->文件路径+名称，value-->整个文件内容
    public class MergeInputFormat extends FileInputFormat<Text, BytesWritable> {


    //重写是否可切分
    @Override
    protected boolean isSplitable(JobContext context, Path filename) {
    //对于当前需求，不需要把文件切分，保证一个切片就是一个文件
    return false;
    }


    //recordReader就是用来读取数据的对象
    @Override
    public RecordReader<Text, BytesWritable> createRecordReader(InputSplit
    split, TaskAttemptContext context) throws IOException, InterruptedException {
    MergeRecordReader recordReader = new MergeRecordReader();
    //调用recordReader的初始化方法
    recordReader.initialize(split, context);
    return recordReader;
    }
    }


package com.lagou.mr.comment.step1;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FSDataInputStream;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.io.IOUtils;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.InputSplit;
import org.apache.hadoop.mapreduce.RecordReader;
import org.apache.hadoop.mapreduce.TaskAttemptContext;
import org.apache.hadoop.mapreduce.lib.input.FileSplit;

import java.io.IOException;

//负责读取数据，一次读取整个文件内容，封装成kv输出
public class MergeRecordReader extends RecordReader<Text, BytesWritable> {
private FileSplit split;
//hadoop配置文件对象
private Configuration conf;

//定义key,value的成员变量
private Text key = new Text();
private BytesWritable value = new BytesWritable();

//初始化方法，把切片以及上下文提升为全局
@Override
public void initialize(InputSplit split, TaskAttemptContext context) throws
IOException, InterruptedException {
this.split = (FileSplit) split;
conf = context.getConfiguration();
}

private Boolean flag = true;

//用来读取数据的方法
@Override
public boolean nextKeyValue() throws IOException, InterruptedException {
//对于当前split来说只需要读取一次即可，因为一次就把整个文件全部读取了。
if (flag) {
//准备一个数组存放读取到的数据，数据大小是多少？
byte[] content = new byte[(int) split.getLength()];
final Path path = split.getPath();//获取切片的path信息
final FileSystem fs = path.getFileSystem(conf);//获取到文件系统对象

final FSDataInputStream fis = fs.open(path); //获取到输入流

IOUtils.readFully(fis, content, 0 , content.length); //读取数据并把数据
放入byte[]
//封装key和value
key.set(path.toString());
value.set(content, 0 , content.length);

IOUtils.closeStream(fis);
//把再次读取的开关置为false

3 、Reducer

4 、Driver

    flag = false;
    return true;
    }


    return false;
    }


    //获取到key
    @Override
    public Text getCurrentKey() throws IOException, InterruptedException {
    return key;
    }


    //获取到value
    @Override
    public BytesWritable getCurrentValue() throws IOException,
    InterruptedException {
    return value;
    }


    //获取进度
    @Override
    public float getProgress() throws IOException, InterruptedException {
    return 0 ;
    }


    //关闭资源
    @Override
    public void close() throws IOException {


    }
    }


    package com.lagou.mr.comment.step1;


    import org.apache.hadoop.io.BytesWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Reducer;


    import java.io.IOException;


    public class MergeReducer extends Reducer<Text, BytesWritable, Text,
    BytesWritable> {
    @Override
    protected void reduce(Text key, Iterable<BytesWritable> values, Context
    context) throws IOException, InterruptedException {
    //输出value值（文件内容），只获取其中第一个即可（只有一个）
    context.write(key, values.iterator().next());
    }
    }


2 、分区排序多目录输出

1 、Mapper

    package com.lagou.mr.comment.step1;


    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.BytesWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;


    import java.io.IOException;


    public class MergeDriver {
    public static void main(String[] args) throws IOException,
    ClassNotFoundException, InterruptedException {


    // 1. 获取配置文件对象，获取job对象实例
    final Configuration conf = new Configuration();


    final Job job = Job.getInstance(conf, "MergeDriver");
    // 2. 指定程序jar的本地路径
    job.setJarByClass(MergeDriver.class);
    // 3. 指定Mapper/Reducer类
    job.setMapperClass(MergeMapper.class);
    job.setReducerClass(MergeReducer.class);
    // 4. 指定Mapper输出的kv数据类型
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(BytesWritable.class);
    // 5. 指定最终输出的kv数据类型
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(BytesWritable.class);


    //设置使用自定义InputFormat读取数据
    job.setInputFormatClass(MergeInputFormat.class);
    FileInputFormat.setInputPaths(job, new Path("E:\\teach\\hadoop框架\\资料
    \\data\\mr综合案例\\input2")); //指定读取数据的原始路径
    job.setOutputFormatClass(SequenceFileOutputFormat.class);
    // 7. 指定job输出结果路径
    FileOutputFormat.setOutputPath(job, new Path("E:\\teach\\hadoop框架\\资料
    \\data\\mr综合案例\\out")); //指定结果数据输出路径
    // 8. 提交作业
    final boolean flag = job.waitForCompletion(true);
    //jvm退出：正常退出 0 ，非 0 值则是错误退出
    System.exit(flag? 0 : 1 );
    }
    }


    package com.lagou.mr.comment.step2;


2 、CommentBean

    import org.apache.commons.lang3.StringUtils;
    import org.apache.hadoop.io.BytesWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;


    import java.io.IOException;


    //第一对kv:使用SequenceFileinputformat读取，所以key:Text,Value:BytesWritable(原因是生
    成sequencefile文件指定就是这种类型)
    public class CommentMapper extends Mapper<Text, BytesWritable, CommentBean,
    NullWritable> {
    //key就是文件名
    //value:一个文件的完整内容
    @Override
    protected void map(Text key, BytesWritable value, Context context) throws
    IOException, InterruptedException {
    //且分区每一行
    String str = new String(value.getBytes());
    String[] lines = str.split("\n");
    for (String line : lines) {
    CommentBean commentBean = parseStrToCommentBean(line);
    if (null != commentBean) {
    context.write(commentBean, NullWritable.get());
    }
    
    }
    
    }
    
    //切分字符串封装成commentbean对象
    public CommentBean parseStrToCommentBean(String line) {
    if (StringUtils.isNotBlank(line)) {
    //每一行进行切分
    String[] fields = line.split("\t");
    if (fields.length >= 9 ) {
    return new CommentBean(fields[ 0 ], fields[ 1 ], fields[ 2 ],
    Integer.parseInt(fields[ 3 ]), fields[ 4 ], fields[ 5 ], fields[ 6 ],
    Integer.parseInt(fields[ 7 ]),
    fields[ 8 ]);
    }
    {
    return null;
    }
    }
    
    return null;
    }
    }
    
    package com.lagou.mr.comment.step2;
    
    import org.apache.hadoop.io.WritableComparable;

import java.io.DataInput;
import java.io.DataOutput;
import java.io.IOException;

public class CommentBean implements WritableComparable<CommentBean> {
private String orderId;
private String comment;
private String commentExt;
private int goodsNum;
private String phoneNum;
private String userName;
private String address;
private int commentStatus;
private String commentTime;

@Override
public String toString() {
return
orderId+"\t"+comment+"\t"+commentExt+"\t"+goodsNum+"\t"+phoneNum+"\t"+userName+"
\t"+address+"\t"+commentStatus+"\t"+commentTime;
}
//无参构造

public CommentBean() {
}

public CommentBean(String orderId, String comment, String commentExt, int
goodsNum, String phoneNum, String userName, String address, int commentStatus,
String commentTime) {
this.orderId = orderId;
this.comment = comment;
this.commentExt = commentExt;
this.goodsNum = goodsNum;
this.phoneNum = phoneNum;
this.userName = userName;
this.address = address;
this.commentStatus = commentStatus;
this.commentTime = commentTime;
}

public String getOrderId() {
return orderId;
}

public void setOrderId(String orderId) {
this.orderId = orderId;
}

public String getComment() {
return comment;
}

public void setComment(String comment) {
this.comment = comment;
}

public String getCommentExt() {

return commentExt;
}

public void setCommentExt(String commentExt) {
this.commentExt = commentExt;
}

public int getGoodsNum() {
return goodsNum;
}

public void setGoodsNum(int goodsNum) {
this.goodsNum = goodsNum;
}

public String getPhoneNum() {
return phoneNum;
}

public void setPhoneNum(String phoneNum) {
this.phoneNum = phoneNum;
}

public String getUserName() {
return userName;
}

public void setUserName(String userName) {
this.userName = userName;
}

public String getAddress() {
return address;
}

public void setAddress(String address) {
this.address = address;
}

public int getCommentStatus() {
return commentStatus;
}

public void setCommentStatus(int commentStatus) {
this.commentStatus = commentStatus;
}

public String getCommentTime() {
return commentTime;
}

public void setCommentTime(String commentTime) {
this.commentTime = commentTime;
}

//定义排序规则,按照时间降序;0,1,-1
@Override
public int compareTo(CommentBean o) {

3 、自定义分区器

4 、自定义OutputFormat

    CommentOutputFormat


    return o.getCommentTime().compareTo(this.commentTime);
    }


    //序列化
    @Override
    public void write(DataOutput out) throws IOException {
    out.writeUTF(orderId);
    out.writeUTF(comment);
    out.writeUTF(commentExt);
    out.writeInt(goodsNum);
    out.writeUTF(phoneNum);
    out.writeUTF(userName);
    out.writeUTF(address);
    out.writeInt(commentStatus);
    out.writeUTF(commentTime);
    }


    //反序列化
    @Override
    public void readFields(DataInput in) throws IOException {
    this.orderId = in.readUTF();
    this.comment = in.readUTF();
    this.commentExt = in.readUTF();
    this.goodsNum = in.readInt();
    this.phoneNum = in.readUTF();
    this.userName = in.readUTF();
    this.address = in.readUTF();
    this.commentStatus = in.readInt();
    this.commentTime = in.readUTF();
    }
    }


    package com.lagou.mr.comment.step2;


    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.mapreduce.Partitioner;


    public class CommentPartitioner extends Partitioner<CommentBean, NullWritable> {
    @Override
    public int getPartition(CommentBean commentBean, NullWritable nullWritable,
    int numPartitions) {
    // return (commentBean.getCommentStatus() & Integer.MAX_VALUE) %
    numPartitions;
    return commentBean.getCommentStatus();//0,1,2 -->对应分区编号的
    }
    }


    package com.lagou.mr.comment.step2;


RecordWriter

    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.FSDataOutputStream;
    import org.apache.hadoop.fs.FileSystem;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.mapreduce.RecordWriter;
    import org.apache.hadoop.mapreduce.TaskAttemptContext;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


    import java.io.IOException;


    //最终输出的kv类型
    public class CommentOutputFormat extends FileOutputFormat<CommentBean,
    NullWritable> {
    //负责写出数据的对象
    @Override
    public RecordWriter<CommentBean, NullWritable>
    getRecordWriter(TaskAttemptContext job) throws IOException,
    InterruptedException {
    Configuration conf = job.getConfiguration();
    FileSystem fs = FileSystem.get(conf);
    //当前reducetask处理的分区编号来创建文件获取输出流
    //获取到在Driver指定的输出路径;0是好评， 1 是中评， 2 是差评
    String outputDir =
    conf.get("mapreduce.output.fileoutputformat.outputdir");
    FSDataOutputStream goodOut=null;
    FSDataOutputStream commonOut=null;
    FSDataOutputStream badOut=null;
    int id = job.getTaskAttemptID().getTaskID().getId();//当前reducetask
    处理的分区编号
    if(id== 0 ){
    //好评数据
    goodOut =fs.create(new Path(outputDir + "\\good\\good.log"));
    }else if(id == 1 ){
    //中评数据
    commonOut = fs.create(new Path(outputDir +
    "\\common\\common.log"));
    }else{
    badOut = fs.create(new Path(outputDir + "\\bad\\bad.log"));
    }


    return new CommentRecorderWrtier(goodOut,commonOut,badOut);


    }
    }


    package com.lagou.mr.comment.step2;


    import org.apache.hadoop.fs.FSDataOutputStream;
    import org.apache.hadoop.io.IOUtils;


5 、Reducer

    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.mapreduce.RecordWriter;
    import org.apache.hadoop.mapreduce.TaskAttemptContext;


    import java.io.IOException;


    public class CommentRecorderWrtier extends RecordWriter<CommentBean,
    NullWritable> {
    //定义写出数据的流
    private FSDataOutputStream goodOut;
    private FSDataOutputStream commonOut;
    private FSDataOutputStream badOut;


    public CommentRecorderWrtier(FSDataOutputStream goodOut,
    FSDataOutputStream commonOut, FSDataOutputStream badOut) {
    this.goodOut = goodOut;
    this.commonOut = commonOut;
    this.badOut = badOut;
    }


    //实现把数据根据不同的评论类型输出到不同的目录下
    //写出数据的逻辑
    @Override
    public void write(CommentBean key, NullWritable value) throws
    IOException, InterruptedException {
    int commentStatus = key.getCommentStatus();
    String beanStr = key.toString();
    if (commentStatus == 0 ) {
    goodOut.write(beanStr.getBytes());
    goodOut.write("\n".getBytes());
    goodOut.flush();
    } else if (commentStatus == 1 ) {
    commonOut.write(beanStr.getBytes());
    commonOut.write("\n".getBytes());
    commonOut.flush();
    } else {
    badOut.write(beanStr.getBytes());
    badOut.write("\n".getBytes());
    badOut.flush();
    }
    }


    //释放资源
    @Override
    public void close(TaskAttemptContext context) throws IOException,
    InterruptedException {
    IOUtils.closeStream(goodOut);
    IOUtils.closeStream(commonOut);
    IOUtils.closeStream(badOut);
    }
    }

6 、Driver

    package com.lagou.mr.comment.step2;
    
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.mapreduce.Reducer;
    
    import java.io.IOException;
    
    public class CommentReducer extends Reducer<CommentBean, NullWritable,
    CommentBean, NullWritable> {
    @Override
    protected void reduce(CommentBean key, Iterable<NullWritable> values,
    Context context) throws IOException, InterruptedException {
    //遍历values，输出的是key；key：是一个引用地址，底层获取value同时，key的值也发生了
    变化
    for (NullWritable value : values) {
    context.write(key, value);
    }
    }
    }
    
    package com.lagou.mr.comment.step2;
    
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    
    import java.io.IOException;
    
    public class CommentDriver {
    public static void main(String[] args) throws IOException,
    ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "CommentDriver");
    job.setJarByClass(CommentDriver.class);


    job.setMapperClass(CommentMapper.class);
    job.setReducerClass(CommentReducer.class);


    job.setMapOutputKeyClass(CommentBean.class);
    job.setMapOutputValueClass(NullWritable.class);


    job.setOutputKeyClass(CommentBean.class);
    job.setOutputValueClass(NullWritable.class);


    job.setPartitionerClass(CommentPartitioner.class);
    //指定inputformat类型
    job.setInputFormatClass(SequenceFileInputFormat.class);
    //指定输出outputformat类型
    job.setOutputFormatClass(CommentOutputFormat.class);
    //指定输入，输出路径


1.4 程序调优

预合并

    CombineMapper


    CombineDriver


    FileInputFormat.setInputPaths(job,
    new Path("E:\\teach\\hadoop框架\\资料\\data\\mr综合案例\\out"));
    FileOutputFormat.setOutputPath(job,
    new Path("E:\\teach\\hadoop框架\\资料\\data\\mr综合案例\\multi-
    out"));
    //指定reducetask的数量
    job.setNumReduceTasks( 3 );
    boolean b = job.waitForCompletion(true);
    if (b) {
    System.exit( 0 );
    }
    }
    }


    package com.lagou.mr.comment.step3;


    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Mapper;


    import java.io.IOException;


    public class CombineMapper extends Mapper<LongWritable,Text,
    NullWritable,Text>{
    @Override
    protected void map(LongWritable key, Text value, Context context) throws
    IOException, InterruptedException {
    context.write(NullWritable.get(), value);
    }
    }


    package com.lagou.mr.comment.step3;


    import com.lagou.mr.comment.step2.*;
    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.LongWritable;
    import org.apache.hadoop.io.NullWritable;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.mapreduce.Job;
    import org.apache.hadoop.mapreduce.lib.input.CombineTextInputFormat;
    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.input.SequenceFileInputFormat;


输出压缩

MergeDriver

    import org.apache.hadoop.mapreduce.lib.input.TextInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;


    import java.io.IOException;


    public class CombineDriver {
    public static void main(String[] args) throws IOException,
    ClassNotFoundException, InterruptedException {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "CombineDriver");
    job.setJarByClass(CombineDriver.class);


    job.setMapperClass(CombineMapper.class);


    job.setMapOutputKeyClass(NullWritable.class);
    job.setMapOutputValueClass(Text.class);


    job.setOutputKeyClass(NullWritable.class);
    job.setOutputValueClass(Text.class);


    //指定inputformat
    job.setInputFormatClass(CombineTextInputFormat.class);
    CombineTextInputFormat.setMaxInputSplitSize(job, 1024 * 1024 * 4 );
    //指定输入，输出路径
    FileInputFormat.setInputPaths(job,
    new Path("E:\\teach\\hadoop框架\\资料\\data\\mr综合案例
    \\input"));
    FileOutputFormat.setOutputPath(job,
    new Path("E:\\teach\\hadoop框架\\资料\\data\\mr综合案例
    \\merge-out"));
    //指定reducetask的数量
    job.setNumReduceTasks( 3 );
    boolean b = job.waitForCompletion(true);
    if (b) {
    System.exit( 0 );
    }
    }
    }


    package com.lagou.mr.comment.step1;


    import org.apache.hadoop.conf.Configuration;
    import org.apache.hadoop.fs.Path;
    import org.apache.hadoop.io.BytesWritable;
    import org.apache.hadoop.io.SequenceFile;
    import org.apache.hadoop.io.Text;
    import org.apache.hadoop.io.compress.CompressionCodec;
    import org.apache.hadoop.io.compress.DefaultCodec;
    import org.apache.hadoop.io.compress.SnappyCodec;
    import org.apache.hadoop.mapreduce.Job;


第七节 MR算法扩展

    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
    import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;
    
    import java.io.IOException;
    
    public class MergeDriver {
    public static void main(String[] args) throws IOException,
    ClassNotFoundException, InterruptedException {
    
    // 1. 获取配置文件对象，获取job对象实例
    final Configuration conf = new Configuration();
    
    final Job job = Job.getInstance(conf, "MergeDriver");
    // 2. 指定程序jar的本地路径
    job.setJarByClass(MergeDriver.class);
    // 3. 指定Mapper/Reducer类
    job.setMapperClass(MergeMapper.class);
    // job.setReducerClass(MergeReducer.class);
    // 4. 指定Mapper输出的kv数据类型
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(BytesWritable.class);
    // 5. 指定最终输出的kv数据类型
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(BytesWritable.class);
    
    //设置使用自定义InputFormat读取数据
    job.setInputFormatClass(MergeInputFormat.class);
    FileInputFormat.setInputPaths(job, new Path("E:\\teach\\hadoop框架\\资料
    \\data\\mr综合案例\\merge-out")); //指定读取数据的原始路径
    //指定输出使用的outputformat
    job.setOutputFormatClass(SequenceFileOutputFormat.class);
    FileOutputFormat.setOutputCompressorClass(job, SnappyCodec.class);
    //record压缩
    SequenceFileOutputFormat.setOutputCompressionType(job,
    SequenceFile.CompressionType.RECORD );
    SequenceFileOutputFormat.setOutputCompressorClass(job,
    DefaultCodec.class);
    // block压缩
    SequenceFileOutputFormat.setOutputCompressionType(job,
    SequenceFile.CompressionType.BLOCK );
    SequenceFileOutputFormat.setOutputCompressorClass(job,
    DefaultCodec.class);
    // 7. 指定job输出结果路径
    FileOutputFormat.setOutputPath(job, new Path("E:\\teach\\hadoop框架\\资料
    \\data\\mr综合案例\\out")); //指定结果数据输出路径
    // 8. 提交作业
    final boolean flag = job.waitForCompletion(true);
    //jvm退出：正常退出 0 ，非 0 值则是错误退出
    System.exit(flag? 0 : 1 );
    }
    }

7.1 MergeSort 归并排序

合并

合并细节

1. 不断地将当前序列平均分割成 2 个子序列

直到不能再分割（序列中只剩 1 个元素）

2. 不断地将 2 个子序列合并成一个有序序列

直到最终只剩下 1 个子序列

时间复杂度：O(nlogn)

空间复杂度：O(n)

7.2 QuickSort-快排

第一步

    从数组中选择一个轴点元素（Pivot element），一般选择 0 位置元素为轴点元素
    第二步


利用Pivot将数组分割成 2 个子序列
将小于 Pivot的元素放在Pivot前面（左侧）
将大于 Pivot的元素放在Pivot后面（右侧）
等于Pivot的元素放哪边都可以(暂定放在左边)
第三步

    对子数组进行第一步，第二步操作，直到不能再分割(子数组中只有一个元素)


时间复杂度

最坏情况：

最好情况：

空间复杂度

    由于递归调用，每次类似折半效果所以空间复杂度是O(logn)


第七部分 YARN资源调度

第一节 Yarn架构

ResourceManager(rm)： 处理客户端请求、启动/监控ApplicationMaster、监控NodeManager、资
源分配与调度；

NodeManager(nm)： 单个节点上的资源管理、处理来自ResourceManager的命令、处理来自
ApplicationMaster的命令；

ApplicationMaster(am)： 数据切分、为应用程序申请资源，并分配给内部任务、任务监控与容错。

Container： 对任务运行环境的抽象，封装了CPU、内存等多维资源以及环境变量、启动命令等任务运
行相关的信息。

第二节 Yarn任务提交(工作机制)

作业提交过程之YARN

作业提交

    第 1 步：Client调用job.waitForCompletion方法，向整个集群提交MapReduce作业。
    第 2 步：Client向RM申请一个作业id。
    第 3 步：RM给Client返回该job资源的提交路径和作业id。
    第 4 步：Client提交jar包、切片信息和配置文件到指定的资源提交路径。
    第 5 步：Client提交完资源后，向RM申请运行MrAppMaster。


作业初始化

    第 6 步：当RM收到Client的请求后，将该job添加到容量调度器中。
    第 7 步：某一个空闲的NM领取到该Job。
    第 8 步：该NM创建Container，并产生MRAppmaster。
    第 9 步：下载Client提交的资源到本地。
    任务分配
    第 10 步：MrAppMaster向RM申请运行多个MapTask任务资源。
    第 11 步：RM将运行MapTask任务分配给另外两个NodeManager，另两个NodeManager分
    别领取任务并创建容器。
    任务运行
    第 12 步：MR向两个接收到任务的NodeManager发送程序启动脚本，这两个NodeManager
    分别启动MapTask，MapTask对数据分区排序。
    第 13 步：MrAppMaster等待所有MapTask运行完毕后，向RM申请容器，运行ReduceTask。
    第 14 步：ReduceTask向MapTask获取相应分区的数据。
    第 15 步：程序运行完毕后，MR会向RM申请注销自己。
    进度和状态更新
    YARN中的任务将其进度和状态返回给应用管理器, 客户端每秒(通过
    mapreduce.client.progressmonitor.pollinterval设置)向应用管理器请求进度更新, 展示给用
    户。
    作业完成
    除了向应用管理器请求作业进度外, 客户端每 5 秒都会通过调用waitForCompletion()来检查作
    业是否完成。时间间隔可以通过mapreduce.client.completion.pollinterval来设置。作业完
    成之后, 应用管理器和Container会清理工作状态。作业的信息会被作业历史服务器存储以备
    之后用户核查。

第三节 Yarn调度策略

Hadoop作业调度器主要有三种：FIFO、Capacity Scheduler和Fair Scheduler。Hadoop2.9.2默认的资
源调度器是Capacity Scheduler。

可以查看yarn-default.xml

1. FIFO(先进先出调度器)

2. 容量调度器（Capacity Scheduler 默认的调度器）

    Apache Hadoop默认使用的调度策略。Capacity 调度器允许多个组织共享整个集群，每个组织可
    以获得集群的一部分计算能力。通过为每个组织分配专门的队列，然后再为每个队列分配一定的集
    群资源，这样整个集群就可以通过设置多个队列的方式给多个组织提供服务了。除此之外，队列内
    部又可以垂直划分，这样一个组织内部的多个成员就可以共享这个队列资源了，在一个队列内部，
    资源的调度是采用的是先进先出(FIFO)策略。

3. Fair Scheduler（公平调度器，CDH版本的hadoop默认使用的调度器）

    Fair调度器的设计目标是为所有的应用分配公平的资源（对公平的定义可以通过参数来设置）。公
    平调度在也可以在多个队列间工作。举个例子，假设有两个用户A和B，他们分别拥有一个队列。
    当A启动一个job而B没有任务时，A会获得全部集群资源；当B启动一个job后，A的job会继续运
    行，不过一会儿之后两个任务会各自获得一半的集群资源。如果此时B再启动第二个job并且其它

    job还在运行，则它将会和B的第一个job共享B这个队列的资源，也就是B的两个job会用于四分之
    一的集群资源，而A的job仍然用于集群一半的资源，结果就是资源最终在两个用户之间平等的共
    享
    

第四节 Yarn多租户资源隔离配置

Yarn集群资源设置为A,B两个队列，

    A队列设置占用资源70%主要用来运行常规的定时任务，
    B队列设置占用资源30%主要运行临时任务，
    两个队列间可相互资源共享，假如A队列资源占满，B队列资源比较充裕，A队列可以使用B队列的
    资源，使总体做到资源利用最大化.
    选择使用Fair Scheduler调度策略！！


具体配置

    1. yarn-site.xml


    2. 创建fair-scheduler.xml文件
    在Hadoop安装目录/etc/hadoop创建该文件


    <!-- 指定我们的任务调度使用fairScheduler的调度方式 -->
    <property>
    <name>yarn.resourcemanager.scheduler.class</name>


    <value>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairSch
    eduler</value>
    <description>In case you do not want to use the default
    scheduler</description>
    </property>


    <?xml version="1.0" encoding="UTF-8" standalone="yes"?>
    <allocations>
    <defaultQueueSchedulingPolicy>fair</defaultQueueSchedulingPolicy>
    <queue name="root" >
    <queue name="default">
    <aclAdministerApps>*</aclAdministerApps>
    <aclSubmitApps>*</aclSubmitApps>
    <maxResources>9216 mb,4 vcores</maxResources>
    <maxRunningApps> 100 </maxRunningApps>
    <minResources>1024 mb,1vcores</minResources>
    <minSharePreemptionTimeout> 1000 </minSharePreemptionTimeout>
    <schedulingPolicy>fair</schedulingPolicy>
    <weight> 7 </weight>
    </queue>
    <queue name="queue1">
    <aclAdministerApps>*</aclAdministerApps>
    <aclSubmitApps>*</aclSubmitApps>
    <maxResources>4096 mb,4vcores</maxResources>
    <maxRunningApps> 5 </maxRunningApps>
    <minResources>1024 mb, 1vcores</minResources>


界面验证

第八部分 Apache Hadoop 核心源码剖析

第一节 源码阅读准备

    1. 下载Apache Hadoop-2.9.2官方源码
    2. 将源码导入idea中
    启动idea在提示界面选择导入


    <minSharePreemptionTimeout> 1000 </minSharePreemptionTimeout>
    <schedulingPolicy>fair</schedulingPolicy>
    <weight> 3 </weight>
    </queue>
    </queue>
    <queuePlacementPolicy>
    <rule create="false" name="specified"/>
    <rule create="true" name="default"/>
    </queuePlacementPolicy>
    </allocations>




等待下载和解决依赖完成，源码导入成功！！

第二节 NameNode 启动流程

命令启动Hdfs集群

该命令会启动Hdfs的NameNode以及DataNode，启动NameNode主要是通过
org.apache.hadoop.hdfs.server.namenode.NameNode类。

我们重点关注NameNode在启动过程中做了哪些工作（偏离主线的技术细节不深究）

对于分析启动流程主要关注两部分代码：

    start-dfs.sh
    
    public class NameNode extends ReconfigurableBase implements
    NameNodeStatusMXBean {
    //该静态代码块主要是初始化一些HDFS的配置信息
    static{
    HdfsConfiguration.init();//进入之后发现方法是空的，没有任何操作？其实不是观察
    HdfsConfiguration的静态代码块
    }
    //HdfsConfiguration的类以及静态代码块
    public class HdfsConfiguration extends Configuration {
    static {
    addDeprecatedKeys();
    
    // adds the default resources
    Configuration.addDefaultResource("hdfs-default.xml");
    Configuration.addDefaultResource("hdfs-site.xml");

}

。。。。

//main方法
public static void main(String argv[]) throws Exception{
//分析传入的参数是否为帮助参数，如果是帮助的话打印帮助信息，并退出。
if(DFSUtil.parseHelpArgument(argv,NameNode.USAGE,System.out,true)){
System.exit( 0 );
}
try{
//格式化输出启动信息，并且创建hook（打印节点关闭信息）
StringUtils.startupShutdownMessage(NameNode.class,argv,LOG);
//创建namenode
NameNode namenode=createNameNode(argv,null);
if（namenode!=null){
//加入集群
namenode.join()
}
}catch(Throwable e){
//异常处理
LOG.error("Failed to start namenode.",e)
terminate( 1 ,e);
}

}

//关注createNameNode
public static NameNode createNameNode(String argv[], Configuration conf)
throws IOException {
LOG.info("createNameNode " + Arrays.asList(argv));
if (conf == null)
conf = new HdfsConfiguration();
// Parse out some generic args into Configuration.
GenericOptionsParser hParser = new GenericOptionsParser(conf, argv);
argv = hParser.getRemainingArgs();
// Parse the rest, NN specific args.
//解析启动的参数
StartupOption startOpt = parseArguments(argv);
if (startOpt == null) {
printUsage(System.err);
return null;
}
setStartupOption(conf, startOpt);

switch (startOpt) {
....
default: { //正常启动进入该分支
//初始化metric系统
DefaultMetricsSystem.initialize("NameNode");
//返回新的NameNode
return new NameNode(conf);
}
}

}

//NameNode的构造
public NameNode(Configuration conf) throws IOException {
this(conf, NamenodeRole.NAMENODE);
}

...

protected NameNode(Configuration conf, NamenodeRole role)
throws IOException {
this.conf = conf;
this.role = role;
// 设置NameNode#clientNamenodeAddress为"hdfs://localhost:9000"
setClientNamenodeAddress(conf);
String nsId = getNameServiceId(conf);
String namenodeId = HAUtil.getNameNodeId(conf, nsId);
// HA相关
this.haEnabled = HAUtil.isHAEnabled(conf, nsId);
state = createHAState(getStartupOption(conf));
this.allowStaleStandbyReads = HAUtil.shouldAllowStandbyReads(conf);
this.haContext = createHAContext();
try {
initializeGenericKeys(conf, nsId, namenodeId);
// 完成实际的初始化工作
initialize(conf);
// HA相关
try {
haContext.writeLock();
state.prepareToEnterState(haContext);
state.enterState(haContext);
} finally {
haContext.writeUnlock();
}
} catch (IOException e) {
this.stop();
throw e;
} catch (HadoopIllegalArgumentException e) {
this.stop();
throw e;
}
}
//尽管本地没有开启HA（haEnabled=false），namenode依然拥有一个HAState，namenode

的HAState状态为active.

// 完成实际的初始化工作
// initialize(conf);
protected void initialize(Configuration conf) throws IOException {
if (conf.get(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS) == null) {
String intervals = conf.get(DFS_METRICS_PERCENTILES_INTERVALS_KEY);
if (intervals != null) {
conf.set(HADOOP_USER_GROUP_METRICS_PERCENTILES_INTERVALS,
intervals);
}
}

UserGroupInformation.setConfiguration(conf);
loginAsNameNodeUser(conf);

// 初始化metric
NameNode.initMetrics(conf, this.getRole());
StartupProgressMetrics.register(startupProgress);

// 启动httpServer
if (NamenodeRole.NAMENODE == role) {

startHttpServer(conf);
}

this.spanReceiverHost = SpanReceiverHost.getInstance(conf);

// 从namenode目录加载fsimage与editlog，初始化FsNamesystem、FsDirectory、
LeaseManager等
loadNamesystem(conf);

// 创建RpcServer，封装了NameNodeRpcServer clientRpcServer，支持
ClientNamenodeProtocol、DatanodeProtocolPB等协议
rpcServer = createRpcServer(conf);
if (clientNamenodeAddress == null) {
// This is expected for MiniDFSCluster. Set it now using
// the RPC server's bind address.
clientNamenodeAddress =
NetUtils.getHostPortString(rpcServer.getRpcAddress());
LOG.info("Clients are to use " + clientNamenodeAddress + " to access"

- " this namenode/service.");
}
if (NamenodeRole.NAMENODE == role) {
httpServer.setNameNodeAddress(getNameNodeAddress());
httpServer.setFSImage(getFSImage());
}

// 启动JvmPauseMonitor等，反向监控JVM
pauseMonitor = new JvmPauseMonitor(conf);
pauseMonitor.start();
metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);

// 启动执行多个非常重要工作的多个线程
startCommonServices(conf);

}

private void startCommonServices(Configuration conf) throws IOException {
// 创建NameNodeResourceChecker、激活BlockManager等
namesystem.startCommonServices(conf, haContext);
registerNNSMXBean();
// 角色非NamenodeRole.NAMENODE的在此处启动HttpServer
if (NamenodeRole.NAMENODE != role) {
startHttpServer(conf);
httpServer.setNameNodeAddress(getNameNodeAddress());
httpServer.setFSImage(getFSImage());
}
// 启动RPCServer
rpcServer.start();
...// 启动各插件
LOG.info(getRole() + " RPC up at: " + rpcServer.getRpcAddress());
if (rpcServer.getServiceRpcAddress() != null) {
LOG.info(getRole() + " service RPC up at: "

- rpcServer.getServiceRpcAddress());
}
}

---

---

namenode的主要责任是文件元信息与数据块映射的管理 。相应的，namenode的启动流程需要关注
与客户端、datanode通信的工作线程，文件元信息的管理机制，数据块的管理机制等。其中，
RpcServer主要负责与客户端、datanode通信，FSDirectory主要负责管理文件元信息。

第三节 DataNode 启动流程

datanode的Main Class是DataNode，先找到DataNode.main()

    void startCommonServices(Configuration conf, HAContext haContext) throws
    IOException {
    this.registerMBean(); // register the MBean for the FSNamesystemState
    writeLock();
    this.haContext = haContext;
    try {
    // 创建NameNodeResourceChecker，并立即检查一次
    nnResourceChecker = new NameNodeResourceChecker(conf);
    checkAvailableResources();
    assert safeMode != null && !isPopulatingReplQueues();
    // 设置一些启动过程中的信息
    StartupProgress prog = NameNode.getStartupProgress();
    prog.beginPhase(Phase.SAFEMODE);
    prog.setTotal(Phase.SAFEMODE, STEP_AWAITING_REPORTED_BLOCKS,
    getCompleteBlocksTotal());
    // 设置已完成的数据块总量
    setBlockTotal();
    // 激活BlockManager
    blockManager.activate(conf);
    } finally {
    writeUnlock();
    }
    
    registerMXBean();
    DefaultMetricsSystem.instance().register(this);
    snapshotManager.registerMXBean();
    }
    //blockManager.activate(conf)激活BlockManager主要完成PendingReplicationMonitor、
    DecommissionManager#Monitor、HeartbeatManager#Monitor、ReplicationMonitor


    public void activate(Configuration conf) {
    // 启动PendingReplicationMonitor
    pendingReplications.start();
    // 激活DatanodeManager：启动DecommissionManager--Monitor、HeartbeatManager--
    Monitor
    datanodeManager.activate(conf);
    // 启动BlockManager--ReplicationMonitor
    this.replicationThread.start();
    }


    public class DataNode extends ReconfigurableBase
    implements InterDatanodeProtocol, ClientDatanodeProtocol,
    TraceAdminProtocol, DataNodeMXBean, ReconfigurationProtocol {
    public static final Logger LOG = LoggerFactory.getLogger(DataNode.class);
    
    static{
    HdfsConfiguration.init();

}

public static void main(String args[]) {
if (DFSUtil.parseHelpArgument(args, DataNode.USAGE, System.out, true)) {
System.exit( 0 );
}
secureMain(args, null);
}
...
public static void secureMain(String args[], SecureResources resources) {
int errorCode = 0 ;
try {
// 打印启动信息
StringUtils.startupShutdownMessage(DataNode.class, args, LOG);
// 完成创建datanode的主要工作
DataNode datanode = createDataNode(args, null, resources);
if (datanode != null) {
datanode.join();
} else {
errorCode = 1 ;
}
} catch (Throwable e) {
LOG.fatal("Exception in secureMain", e);
terminate( 1 , e);
} finally {
LOG.warn("Exiting Datanode");
terminate(errorCode);
}

}

public static DataNode createDataNode(String args[], Configuration conf,
SecureResources resources) throws IOException {
// 完成大部分初始化的工作，并启动部分工作线程
DataNode dn = instantiateDataNode(args, conf, resources);
if (dn != null) {
// 启动剩余工作线程
dn.runDatanodeDaemon();
}
return dn;

}

/** Start a single datanode daemon and wait for it to finish.

- If this thread is specifically interrupted, it will stop waiting.
*/
public void runDatanodeDaemon() throws IOException {
// 在DataNode.instantiateDataNode()执行过程中会调用该方法（见后）
blockPoolManager.startAll();
dataXceiverServer.start();
if (localDataXceiverServer != null) {
localDataXceiverServer.start();
}
ipcServer.start();
startPlugins(conf);
}

---

public static DataNode instantiateDataNode(String args [], Configuration
conf,
SecureResources resources) throws IOException {
if (conf == null)

conf = new HdfsConfiguration();

... // 参数检查等

Collection<StorageLocation> dataLocations = getStorageLocations(conf);
UserGroupInformation.setConfiguration(conf);
SecurityUtil.login(conf, DFS_DATANODE_KEYTAB_FILE_KEY,
DFS_DATANODE_KERBEROS_PRINCIPAL_KEY);
return makeInstance(dataLocations, conf, resources);

}

//DataNode.makeInstance()开始创建DataNode
static DataNode makeInstance(Collection<StorageLocation> dataDirs,
Configuration conf, SecureResources resources) throws IOException {
...// 检查数据目录的权限
assert locations.size() > 0 : "number of data directories should be > 0";
return new DataNode(conf, locations, resources);
}
...
DataNode(final Configuration conf,
final List<StorageLocation> dataDirs,
final SecureResources resources) throws IOException {
super(conf);
...// 参数设置

try {
hostName = getHostName(conf);
LOG.info("Configured hostname is " + hostName);
startDataNode(conf, dataDirs, resources);
} catch (IOException ie) {
shutdown();
throw ie;
}
}
...
void startDataNode(Configuration conf,
List<StorageLocation> dataDirs,
SecureResources resources
) throws IOException {
...// 参数设置

// 初始化DataStorage
storage = new DataStorage();

// global DN settings
// 注册JMX
registerMXBean();
// 初始化DataXceiver（流式通信），DataNode runDatanodeDaemon()中启动
initDataXceiver(conf);
// 启动InfoServer（Web UI）
startInfoServer(conf);
// 启动JVMPauseMonitor（反向监控JVM情况，可通过JMX查询）
pauseMonitor = new JvmPauseMonitor(conf);
pauseMonitor.start();
...// 略

// 初始化IpcServer（RPC通信），DataNode-runDatanodeDaemon()中启动
initIpcServer(conf);

metrics = DataNodeMetrics.create(conf, getDisplayName());
metrics.getJvmMetrics().setPauseMonitor(pauseMonitor);

// 按照namespace（nameservice）、namenode的结构进行初始化
blockPoolManager = new BlockPoolManager(this);
blockPoolManager.refreshNamenodes(conf);

...// 略
}
//BlockPoolManager抽象了datanode提供的数据块存储服务。BlockPoolManager按照
namespace（nameservice）、namenode结构组织。
//BlockPoolManager-refreshNamenodes()
//除了初始化过程主动调用，还可以由namespace通过datanode心跳过程下达刷新命令
void refreshNamenodes(Configuration conf)
throws IOException {
LOG.info("Refresh request received for nameservices: " + conf.get
(DFSConfigKeys.DFS_NAMESERVICES));
Map<String, Map<String, InetSocketAddress>> newAddressMap = DFSUtil
.getNNServiceRpcAddressesForCluster(conf);
synchronized (refreshNamenodesLock) {
doRefreshNamenodes(newAddressMap);
}

}

private void doRefreshNamenodes(
Map<String, Map<String, InetSocketAddress>> addrMap) throws IOException {
assert Thread.holdsLock(refreshNamenodesLock);
Set<String> toRefresh = Sets.newLinkedHashSet();
Set<String> toAdd = Sets.newLinkedHashSet();
Set<String> toRemove;

synchronized (this) {
// Step 1. For each of the new nameservices, figure out whether
// it's an update of the set of NNs for an existing NS,
// or an entirely new nameservice.
for (String nameserviceId : addrMap.keySet()) {
if (bpByNameserviceId.containsKey(nameserviceId)) {
toRefresh.add(nameserviceId);
} else {
toAdd.add(nameserviceId);
}
}

...// 略

// Step 2. Start new nameservices
if (!toAdd.isEmpty()) {
LOG.info("Starting BPOfferServices for nameservices: " +
Joiner.on(",").useForNull("<default>").join(toAdd));

for (String nsToAdd : toAdd) {
ArrayList<InetSocketAddress> addrs =
Lists.newArrayList(addrMap.get(nsToAdd).values());
// 为每个namespace创建对应的BPOfferService
BPOfferService bpos = createBPOS(addrs);
bpByNameserviceId.put(nsToAdd, bpos);
offerServices.add(bpos);
}

}

// 然后通过startAll启动所有BPOfferService
startAll();
}

...// 略

}

protected BPOfferService createBPOS(List<InetSocketAddress> nnAddrs) {
return new BPOfferService(nnAddrs, dn);
}

BPOfferService(List<InetSocketAddress> nnAddrs, DataNode dn) {
Preconditions.checkArgument(!nnAddrs.isEmpty(),
"Must pass at least one NN.");
this.dn = dn;
for (InetSocketAddress addr : nnAddrs) {
this.bpServices.add(new BPServiceActor(addr, this));
}

}

//BlockPoolManager#startAll()启动所有BPOfferService（实际是启动所有
BPServiceActor）。
synchronized void startAll() throws IOException {
try {
UserGroupInformation.getLoginUser().doAs(
new PrivilegedExceptionAction<Object>() {
@Override
public Object run() throws Exception {
for (BPOfferService bpos : offerServices) {
bpos.start();
}
return null;
}
});
} catch (InterruptedException ex) {
IOException ioe = new IOException();
ioe.initCause(ex.getCause());
throw ioe;
}

}

//在datanode启动的主流程中，启动了多种工作线程，包括InfoServer、JVMPauseMonitor、
BPServiceActor等。其中，最重要的是BPServiceActor线程，真正代表datanode与namenode通信的
正是BPServiceActor线程。

//DataNode--initBlockPool()：
/**

- One of the Block Pools has successfully connected to its NN.
- This initializes the local storage for that block pool,
- checks consistency of the NN's cluster ID, etc.
*
- If this is the first block pool to register, this also initializes
- the datanode-scoped storage.
*
- @param bpos Block pool offer service
- @throws IOException if the NN is inconsistent with the local storage.
*/

第四节 NameNode如何支撑高并发访问(双缓冲机制)

高并发访问NameNode会遇到什么样的问题：

经过学习HDFS的元数据管理机制，Client每次请求NameNode修改一条元数据（比如说申请上传一个
文件，都要写一条edits log，包括两个步骤：

    写入本地磁盘--edits文件
    通过网络传输给JournalNodes集群(Hadoop HA集群--结合zookeeper来学习)。


高并发的难点主要在于数据的多线程安全以及每个操作效率！！

对于多线程安全：

NameNode在写edits log时几个原则：

    写入数据到edits_log必须保证每条edits都有一个全局顺序递增的transactionId（简称为txid），
    这样才可以标识出来一条一条的edits的先后顺序。
    如果要保证每条edits的txid都是递增的，就必须得加同步锁。也就是每个线程修改了元数据，要写
    一条edits 的时候，都必须按顺序排队获取锁后，才能生成一个递增的txid，代表这次要写的edits
    的序号。


产生的问题：

如果每次都是在一个加锁的代码块里，生成txid，然后写磁盘文件edits log，这种既有同步锁又有写磁
盘操作非常耗时！！

HDFS优化解决方案

问题产生的原因主要是在于，写edits时串行化排队生成自增txid + 写磁盘操作费时，

HDFS的解决方案

    1. 串行化：使用分段锁
    2. 写磁盘：使用双缓冲


分段加锁机制
首先各个线程依次第一次获取锁，生成顺序递增的txid，然后将edits写入内存双缓冲的区域 1 ，接着就
立马第一次释放锁了。趁着这个空隙，后面的线程就可以再次立马第一次获取锁，然后立即写自己的
edits到内存缓冲。

双缓冲机制

    void initBlockPool(BPOfferService bpos) throws IOException {
    ...// 略
    // 将blockpool注册到BlockPoolManager
    blockPoolManager.addBlockPool(bpos);


    // 初步初始化存储结构
    initStorage(nsInfo);
    ...// 检查磁盘损坏
    // 启动扫描器
    initPeriodicScanners(conf);


    // 将blockpool添加到FsDatasetIpml，并继续初始化存储结构
    data.addBlockPool(nsInfo.getBlockPoolID(), conf);
    }


程序中将会开辟两份一模一样的内存空间，一个为bufCurrent，产生的数据会直接写入到这个
bufCurrent，而另一个叫bufReady，在bufCurrent数据写入(达到一定标准)后，两片内存就会
exchange（交换）。直接交换双缓冲的区域 1 和区域 2 。保证接收客户端写入数据请求的都是操作内存
而不是同步写磁盘。

双缓冲源码分析 找到FsEditLog.java

扩展 Hadoop 3.x 新特性概述

....

    void logEdit(final FSEditLogOp op) {
    boolean needsSync = false;//是否同步的标识
    synchronized (this) {//
    assert isOpenForWrite() :
    "bad state: " + state;
    
    // wait if an automatic sync is scheduled 如果当前操作被其它线程调度，则等待1s钟
    waitIfAutoSyncScheduled();
    
    // check if it is time to schedule an automatic sync
    needsSync = doEditTransaction(op);
    if (needsSync) {
    isAutoSyncScheduled = true;//标识bufCurrent满了，进行双缓冲刷写
    }
    }
    
    // Sync the log if an automatic sync is required.
    if (needsSync) {
    logSync();//将缓冲区数据刷写到磁盘
    }
    }
    
    ...

Hadoop3.x中增强了很多特性，在Hadoop3.x中，不再允许使用jdk1.7，要求jdk1.8以上版本。这是因
为Hadoop 2.0是基于JDK 1.7开发的，而JDK 1.7在 2015 年 4 月已停止更新，这直接迫使Hadoop社区基
于JDK 1.8重新发布一个新的Hadoop版本，而这正是Hadoop3.x。Hadoop3.x以后将会调整方案架构，
将Mapreduce 基于内存+io+磁盘，共同处理数据。

Hadoop 3.x中引入了一些重要的功能和优化，包括HDFS 可擦除编码、多Namenode支持、MR Native
Task优化、YARN基于cgroup的内存和磁盘IO隔离、YARN container resizing等。

Hadoop3.x官方文档地址如下：

第一节 Hadoop3.x新特性之Common改进

Hadoop Common改进：

    1. 精简Hadoop内核，包括剔除过期的API和实现，将默认组件实现替换成最高效的实现（比如将
    FileOutputCommitter缺省实现换为v2版本，废除hftp转由webhdfs替代，移除Hadoop子实现序
    列化库org.apache.hadoop.Records
    2. lasspath isolation以防止不同版本jar包冲突，比如google Guava在混合使用Hadoop、HBase和
    Spark时，很容易产生冲突。（https://issues.apache.org/jira/browse/HADOOP-11656）
    3. Shell脚本重构。 Hadoop 3.0对Hadoop的管理脚本进行了重构，修复了大量bug，增加了新特
    性，支持动态命令等。使用方式上则和之前版本的一致。（https://issues.apache.org/jira/brows
    e/HADOOP-9902）Hadoop3.x新特性之HDFS改进

Hadoop3.x中最大改变的是HDFS，HDFS通过最近black块计算，根据最近计算原则，本地black块，加
入到内存，先计算，通过IO，共享内存计算区域，最后快速形成计算结果。

    1. HDFS支持数据的擦除编码，这使得HDFS在不降低可靠性的前提下，节省一半存储空间。（http
    s://issues.apache.org/jira/browse/HDFS-7285）
    2. 多NameNode支持，即支持一个集群中，一个active、多个standby namenode部署方式。注：
    多ResourceManager特性在hadoop 2.0中已经支持。（https://issues.apache.org/jira/browse/
    HDFS-6440）


关于这两个特性的官方文档地址：

第二节 Hadoop3.x新特性之YARN改进

    1. 基于cgroup的内存隔离和IO Disk隔离（https://issues.apache.org/jira/browse/YARN-2619）
    2. 用curator实现RM leader选举（https://issues.apache.org/jira/browse/YARN-4438）
    3. containerresizing（https://issues.apache.org/jira/browse/YARN-1197）
    4. Timelineserver next generation （https://issues.apache.org/jira/browse/YARN-2928）


官方文档地址：

第三节 Hadoop3.x新特性之MapReduce改进

    http://hadoop.apache.org/docs/r3.0.1/


    http://hadoop.apache.org/docs/r3.0.1/hadoop-project-dist/hadoop-
    hdfs/HDFSErasureCoding.html
    http://hadoop.apache.org/docs/r3.0.1/hadoop-project-dist/hadoop-
    hdfs/HDFSHighAvailabilityWithQJM.html


    http://hadoop.apache.org/docs/r3.0.1/hadoop-yarn/hadoop-yarn-
    site/TimelineServiceV2.html


    1. Tasknative优化。为MapReduce增加了C/C++的map output collector实现（包括Spill，Sort和
    IFile等），通过作业级别参数调整就可切换到该实现上。对于shuffle密集型应用，其性能可提高
    约30%。（https://issues.apache.org/jira/browse/MAPREDUCE-2841）
    2. MapReduce内存参数自动推断。在Hadoop 2.0中，为MapReduce作业设置内存参数非常繁琐，
    涉及到两个参数：mapreduce.{map,reduce}.memory.mb和mapreduce.
    {map,reduce}.java.opts，一旦设置不合理，则会使得内存资源浪费严重，比如将前者设置为
    4096MB，但后者却是“-Xmx2g”，则剩余2g实际上无法让java heap使用到。（https://issues.apa
    che.org/jira/browse/MAPREDUCE-5785）
    Hadoop3.x新特性之其他
    3. 添加新的 hadoop-client-api 和 hadoop-client-runtime 组件到一个单独的jar包里，以此解决依赖
    不兼容的问题。 （https://issues.apache.org/jira/browse/HADOOP-11804）
    4. 支持微软的Azure分布式文件系统和阿里的aliyun分布式文件系统


第九部分 调优及二次开发示例

第一节 Job执行三原则

充分利用集群资源

    reduce阶段尽量放在一轮
    每个task的执行时间要合理


1.1 原则一 充分利用集群资源

Job运行时，尽量让所有的节点都有任务处理，这样能尽量保证集群资源被充分利用，任务的并发度
达到最大。可以通过调整处理的数据量大小，以及调整map和reduce个数来实现。

    Reduce个数的控制使用“mapreduce.job.reduces”
    Map个数取决于使用了哪种InputFormat，默认的TextFileInputFormat将根据block的个数来分配
    map数(一个block一个map)。


1.2 原则二 ReduceTask并发调整

努力避免出现以下场景

    观察Job如果大多数ReduceTask在第一轮运行完后，剩下很少甚至一个ReduceTask刚开始运行。
    这种情况下，这个ReduceTask的执行时间将决定了该job的运行时间。可以考虑将reduce个数减
    少。
    观察Job的执行情况如果是MapTask运行完成后，只有个别节点有ReduceTask在运行。这时候集
    群资源没有得到充分利用，需要增加Reduce的并行度以便每个节点都有任务处理。


1.3 原则三 Task执行时间要合理

一个job中，每个MapTask或ReduceTask的执行时间只有几秒钟，这就意味着这个job的大部分时间
都消耗在task的调度和进程启停上了，因此可以考虑增加每个task处理的数据大小。建议一个task处理
时间为 1 分钟。

第二节 Shuffle调优

Shuffle阶段是MapReduce性能的关键部分，包括了从MapTaskask将中间数据写到磁盘一直到
ReduceTask拷贝数据并最终放到Reduce函数的全部过程。这一块Hadoop提供了大量的调优参数。

2.1 Map阶段

1 、判断Map内存使用

判断Map分配的内存是否够用，可以查看运行完成的job的Counters中(历史服务器)，对应的task是
否发生过多次GC，以及GC时间占总task运行时间之比。通常，GC时间不应超过task运行时间的10%，
即GC time elapsed (ms)/CPU time spent (ms)<10%。

Map需要的内存还需要随着环形缓冲区的调大而对应调整。可以通过如下参数进行调整。

Ma需要的CPU核数可以通过如下参数调整

可以看到内存默认是1G，CPU默认是 1 核。

如果集群资源充足建议调整：

mapreduce.map.memory.mb =3G（默认1G） mapreduce.map.cpu.vcores =1(默认也是1)

    环形缓冲区
    
    mapreduce.map.memory.mb
    
    mapreduce.map.cpu.vcores
    
    Map方法执行后首先把数据写入环形缓冲区，为什么MR框架选择先写内存而不是直接写磁盘？这
    样的目的主要是为了减少磁盘i/o
    环形缓冲默认100M（ mapreduce.task.io.sort.mb ），当到达80%
    （ mapreduce.map.sort.spill.percent ）时就会溢写磁盘。
    每达到80%都会重写溢写到一个新的文件。

当集群内存资源充足，考虑增大 mapreduce.task.io.sort.mb 提高溢写的效率，而且会减少中间结
果的文件数量。

建议：

    调整 mapreduce.task.io.sort.mb =512M。
    当文件溢写完后，会对这些文件进行合并，默认每次合并
    10 （ mapreduce.task.io.sort.factor ）个溢写的文件，建议调整
    mapreduce.task.io.sort.factor =64。这样可以提高合并的并行度，减少合并的次数，降低对磁
    盘操作的次数。


2 、Combiner

在Map阶段，有一个可选过程，将同一个key值的中间结果合并，叫做Combiner。(一般将reduce类
设置为combiner即可)

通过Combine，一般情况下可以显著减少Map输出的中间结果，从而减少shuffle过程的网络带宽占
用。

建议：

不影响最终结果的情况下，加上Combiner!!

2.2 Copy阶段

    对Map的中间结果进行压缩，当数据量大时，会显著减少网络传输的数据量，
    但是也因为多了压缩和解压，带来了更多的CPU消耗。因此需要做好权衡。当任务属于网络瓶颈类
    型时，压缩Map中间结果效果明显。
    在实际经验中Hadoop的运行的瓶颈一般都是IO而不是CPU，压缩一般可以 10 倍的减少IO操作

2.3 Reduce阶段

1 、Reduce资源

每个Reduce资源

2 、Copy

ReduceTask在copy的过程中默认使用 5 （ mapreduce.reduce.shuffle.parallelcopies 参数控制）个
并行度进行复制数据。

该值在实际服务器上比较小，建议调整为50-100.

3 、溢写归并

Copy过来的数据会先放入内存缓冲区中，然后当使用内存达到一定量的时候spill磁盘。这里的缓冲区
大小要比map端的更为灵活，它基于JVM的heap size设置。这个内存大小的控制是通过
mapreduce.reduce.shuffle.input.buffer.percent （default 0.7）控制的。

    mapreduce.reduce.memory.mb=5G（默认1G）
    mapreduce.reduce.cpu.vcores= 1 （默认为 1 ）。

shuffile在reduce内存中的数据最多使用内存量为：0.7 × maxHeap of reduce task，内存到磁盘
merge的启动可以通过 mapreduce.reduce.shuffle.merge.percent （default0.66）配置。

copy完成后，reduce进入归并排序阶段，合并因子默认为 10 （ mapreduce.task.io.sort.factor 参数
控制），如果map输出很多，则需要合并很多趟，所以可以提高此参数来减少合并次数。

第三节 Job调优

1 、推测执行

集群规模很大时（几百上千台节点的集群），个别机器出现软硬件故障的概率就变大了，并且会因此

延长整个任务的执行时间推测执行通过将一个task分给多台机器跑，取先运行完的那个，会很好的解决
这个问题。对于小集群，可以将这个功能关闭。

建议：

大型集群建议开启，小集群建议关闭！

集群的推测执行都是关闭的。在需要推测执行的作业执行的时候开启

2 、Slow Start

MapReduce的AM在申请资源的时候，会一次性申请所有的Map资源，延后申请reduce的资源，这
样就能达到先执行完大部分Map再执行Reduce的目的。

mapreduce.job.reduce.slowstart.completedmaps

当多少占比的Map执行完后开始执行Reduce。默认5%的Map跑完后开始起Reduce。

如果想要Map完全结束后执行Reduce调整该值为 1

4 、小文件优化

    HDFS：hadoop的存储每个文件都会在NameNode上记录元数据，如果同样大小的文件，文件很
    小的话，就会产生很多文件，造成NameNode的压力。
    MR：Mapreduce中一个map默认处理一个分片或者一个小文件，如果map的启动时间都比数据
    处理的时间还要长，那么就会造成性能低，而且在map端溢写磁盘的时候每一个map最终会产生
    reduce数量个数的中间结果，如果map数量特别多，就会造成临时文件很多，而且在reduce拉取
    数据的时候增加磁盘的IO。


如何处理小文件？

    从源头解决，尽量在HDFS上不存储小文件，也就是数据上传HDFS的时候就合并小文件
    通过运行MR程序合并HDFS上已经存在的小文件
    MR计算的时候可以使用CombineTextInputFormat来降低MapTask并行度


    mapreduce.reduce.shuffle.parallelcopies #复制数据的并行度，默认 5 ；建议调整为50-100
    mapreduce.task.io.sort.factor  #一次合并文件个数，默认 10 ，建议调整为 64
    mapreduce.reduce.shuffle.input.buffer.percent #在shuffle的复制阶段，分配给Reduce输出
    缓冲区占堆内存的百分比，默认0.7
    mapreduce.reduce.shuffle.merge.percent #Reduce输出缓冲区的阈值，用于启动合并输出和磁盘
    溢写的过程


5 、数据倾斜

MR是一个并行处理的任务，整个Job花费的时间是作业中所有Task最慢的那个了。

为什么会这样呢？为什么会有的Task快有的Task慢？

    数据倾斜，每个Reduce处理的数据量不是同一个级别的，所有数据量少的Task已经跑完了，数据
    量大的Task则需要更多时间。
    有可能就是某些作业所在的NodeManager有问题或者container有问题，导致作业执行缓慢。


数据倾斜

那么为什么会产生数据倾斜呢？

数据本身就不平衡，所以在默认的hashpartition时造成分区数据不一致问题

那如何解决数据倾斜的问题呢？

    默认的是hash算法进行分区，我们可以尝试自定义分区，修改分区实现逻辑，结合业务特点，使
    得每个分区数据基本平衡
    可以尝试修改分区的键，让其符合hash分区，并且使得最后的分区平衡，比如在key前加随机数n-
    key。
    抽取导致倾斜的key对应的数据单独处理。


如果不是数据倾斜带来的问题，而是节点服务有问题造成某些map和reduce执行缓慢呢？

使用推测执行找个其他的节点重启一样的任务竞争，谁快谁为准。推测执行时以空间换时间的优化。
会带来集群资源的浪费，会给集群增加压力。

第四节 YARN调优

1 、NM配置

可用内存

刨除分配给操作系统、其他服务的内存外，剩余的资源应尽量分配给YARN。

    默认情况下，Map或Reduce container会使用 1 个虚拟CPU内核和1024MB内存，
    ApplicationMaster使用1536MB内存。


CPU虚拟核数

建议将此配置设定在逻辑核数的1.5～ 2 倍之间。如果CPU的计算能力要求不高，可以配置为 2 倍的

逻辑CPU。

2 、Container启动模式

YARN的NodeManager提供 2 种Container的启动模式。

默认，YARN为每一个Container启动一个JVM，JVM进程间不能实现资源共享，导致资源本地化的时间
开销较大。针对启动时间较长的问题，新增了基于线程资源本地化启动模式，能够有效提升container
启动效率。

    yarn.nodemanager.resource.memory-mb 默认是 8192


    yarn.nodemanager.resource.cpu-vcores
    该节点上YARN可使用的虚拟CPU个数，默认是 8 。
    目前推荐将该值设值为逻辑CPU核数的1.5～ 2 倍之间


    设置为“org.apache.hadoop.yarn.server.nodemanager.DefaultContainerExecutor”，则每次启
    动container将会启动一个线程来实现资源本地化。
    该模式下，启动时间较短，但无法做到资源（CPU、内存）隔离。
    设置为“org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor” ，则每次启动
    container都会启动一个JVM进程来实现资源本地化。
    该模式下，启动时间较长，但可以提供较好的资源（CPU、内存）隔离能力。


3 、AM调优

运行的一个大任务，map总数达到了上万的规模，任务失败，发现是ApplicationMaster（以下简称
AM）反应缓慢，最终超时失败。

失败原因是Task数量变多时，AM管理的对象也线性增长，因此就需要更多的内存来管理。AM默认分配
的内存大小是1.5GB。

建议：

任务数量多时增大AM内存

第五节 Namenode Full GC

JVM堆内存

    JVM内存划分为堆内存和非堆内存，堆内存分为年轻代（Young Generation）、老年代（Old
    Generation），非堆内存就一个永久代（Permanent Generation）。
    
    yarn.nodemanager.container-executor.class
    
    yarn.app.mapreduce.am.resource.mb
    
    年轻代又分为Eden和Survivor区。Survivor区由FromSpace和ToSpace组成。Eden区占大容量，
    Survivor两个区占小容量，默认比例是8:1:1。
    堆内存用途：存放的是对象，垃圾收集器就是收集这些对象，然后根据GC算法回收。
    非堆内存用途：永久代，也称为方法区，存储程序运行时长期存活的对象，比如类的元数据、方
    法、常量、属性等。

补充：

JDK1.8版本废弃了永久代，替代的是元空间（MetaSpace），元空间与永久代上类似，都是方法区的实
现，他们最大区别是：元空间并不在JVM中，而是使用本地内存。

1 、对象分代

    新生成的对象首先放到年轻代Eden区，
    当Eden空间满了，触发Minor GC，存活下来的对象移动到Survivor0区，
    Survivor0区满后触发执行Minor GC，Survivor0区存活对象移动到Suvivor1区，这样保证了一段
    时间内总有一个survivor区为空。
    经过多次Minor GC仍然存活的对象移动到老年代。
    老年代存储长期存活的对象，占满时会触发Major GC（Full GC），GC期间会停止所有线程等待
    GC完成，所以对响应要求高的应用尽量减少发生Major GC，避免响应超时。

Minor GC ： 清理年轻代
Major GC(Full GC) ： 清理老年代，清理整个堆空间，会停止应用所有线程。

2 、Jstat

查看当前jvm内存使用以及垃圾回收情况

结果解释：

    jstat  -gc -t 58563 1s #显示pid是 58563 的垃圾回收堆的行为统计
    Timestamp S0C S1C S0U S1U EC EU OC OU
    MC MU CCSC CCSU YGC YGCT FGC FGCT GCT
    9751 .8 12288 .0 12288 .0 0 .0 0 .0 158208 .0 8783 .6 54272 .0
     23264 .6 35496 .0 34743 .9 4144 .0 3931 .8 9  0 .231 2  0 .123 0 .354
    9752 .8 12288 .0 12288 .0 0 .0 0 .0 158208 .0 8783 .6 54272 .0
     23264 .6 35496 .0 34743 .9 4144 .0 3931 .8 9  0 .231 2  0 .123 0 .354
    9753 .8 12288 .0 12288 .0 0 .0 0 .0 158208 .0 8783 .6 54272 .0
     23264 .6 35496 .0 34743 .9 4144 .0 3931 .8 9  0 .231 2  0 .123 0 .354
    9754 .8 12288 .0 12288 .0 0 .0 0 .0 158208 .0 8783 .6 54272 .0
     23264 .6 35496 .0 34743 .9 4144 .0 3931 .8 9  0 .231 2  0 .123 0 .354
    9755 .8 12288 .0 12288 .0 0 .0 0 .0 158208 .0 8783 .6 54272 .0
     23264 .6 35496 .0 34743 .9 4144 .0 3931 .8 9  0 .231 2  0 .123 0 .354
    9756 .9 12288 .0 12288 .0 0 .0 0 .0 158208 .0 8783 .6 54272 .0
     23264 .6 35496 .0 34743 .9 4144 .0 3931 .8 9  0 .231 2  0 .123 0 .354
    
    #C即Capacity 总容量，U即Used 已使用的容量
    S0C: 当前survivor0区容量（kB）。
    S1C: 当前survivor1区容量（kB）。
    S0U: survivor0区已使用的容量（KB）
    S1U: survivor1区已使用的容量（KB）


开启HDFS GC详细日志输出

编辑hadoop-env.sh

增加JMX配置打印详细GC信息

指定一个日志输出目录；注释掉之前的ops

增加新的打印配置

    EC: Eden区的总容量（KB）
    EU: 当前Eden区已使用的容量（KB）
    OC: Old空间容量（kB）。
    OU: Old区已使用的容量（KB）
    MC: Metaspace空间容量（KB）
    MU: Metacspace使用量（KB）
    CCSC: 压缩类空间容量（kB）。
    CCSU: 压缩类空间使用（kB）。
    YGC: 新生代垃圾回收次数
    YGCT: 新生代垃圾回收时间
    FGC: 老年代 full GC垃圾回收次数
    FGCT: 老年代垃圾回收时间
    GCT: 垃圾回收总消耗时间


    export HADOOP_LOG_DIR=/hadoop/logs/


#JMX配置

    export HADOOP_JMX_OPTS="-Dcom.sun.management.jmxremote.authenticate=false -
    Dcom.sun.management.jmxremote.ssl=false"
    export HADOOP_NAMENODE_OPTS="-Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-
    INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender}
    $HADOOP_NAMENODE_OPTS"
    export HADOOP_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS
    $HADOOP_DATANODE_OPTS"
    
    export NAMENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -
    Xloggc:${HADOOP_LOG_DIR}/logs/hadoop-gc.log \
    -XX:+PrintGCDateStamps -XX:+PrintGCApplicationConcurrentTime -
    XX:+PrintGCApplicationStoppedTime \
    -server -Xms150g -Xmx150g -Xmn20g -XX:SurvivorRatio=8 -
    XX:MaxTenuringThreshold=15 \
    -XX:ParallelGCThreads=18 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -
    XX:+UseCMSCompactAtFullCollection -XX:+DisableExplicitGC -
    XX:+CMSParallelRemarkEnabled \
    -XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=70 -
    XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -
    XX:CMSMaxAbortablePrecleanTime=5000 \
    -XX:+UseGCLogFileRotation -XX:GCLogFileSize=20m -
    XX:ErrorFile=${HADOOP_LOG_DIR}/logs/hs_err.log.%p -
    XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${HADOOP_LOG_DIR}/logs/%p.hprof
    \
    "
    
    export DATENODE_OPTS="-verbose:gc -XX:+PrintGCDetails -
    Xloggc:${HADOOP_LOG_DIR}/hadoop-gc.log \

- Xms150g -Xmx150g：堆内存大小最大和最小都是150g
- Xmn20g：新生代大小为20g，等于eden+2*survivor，意味着老年代为150-20=130g。
- XX:SurvivorRatio=8：Eden和Survivor的大小比值为 8 ，意味着两个Survivor区和一个Eden区
的比值为 2 ： 8 ，一个Survivor占整个年轻代的1/10
- XX:ParallelGCThreads=10：设置ParNew GC的线程并行数，默认为8 +
(Runtime.availableProcessors - 8) * 5/8， 24 核机器为 18 。
- XX:MaxTenuringThreshold=15：设置对象在年轻代的最大年龄，超过这个年龄则会晋升到老年
代
- XX:+UseParNewGC：设置新生代使用Parallel New GC
- XX:+UseConcMarkSweepGC：设置老年代使用CMS GC，当此项设置时候自动设置新生代为
ParNew GC
- XX:CMSInitiatingOccupancyFraction=70：
老年代第一次占用达到该百分比时候，就会引发CMS的第一次垃圾回收周期。后继CMS GC由
HotSpot自动优化计算得到。

3 、GC 日志解析

jstat命令输出

    -XX:+PrintGCDateStamps -XX:+PrintGCApplicationConcurrentTime -
    XX:+PrintGCApplicationStoppedTime \
    -server -Xms15g -Xmx15g -Xmn4g -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=15
    \
    -XX:ParallelGCThreads=18 -XX:+UseConcMarkSweepGC -XX:+UseParNewGC -
    XX:+UseCMSCompactAtFullCollection -XX:+DisableExplicitGC -
    XX:+CMSParallelRemarkEnabled \
    -XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=70 -
    XX:+UseFastAccessorMethods -XX:+UseCMSInitiatingOccupancyOnly -
    XX:CMSMaxAbortablePrecleanTime=5000 \
    -XX:+UseGCLogFileRotation -XX:GCLogFileSize=20m -
    XX:ErrorFile=${HADOOP_LOG_DIR}/logs/hs_err.log.%p -
    XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${HADOOP_LOG_DIR}/logs/%p.hprof
    \
    "


    export HADOOP_NAMENODE_OPTS="$NAMENODE_OPTS $HADOOP_NAMENODE_OPTS"
    export HADOOP_DATANODE_OPTS="$DATENODE_OPTS $HADOOP_DATANODE_OPTS"


查看GC日志输出

    ime S0C S1C S0U S1U EC EU OC OU MC
    MU CCSC CCSU YGC YGCT FGC FGCT GCT
    5 .7 34048 .0 34048 .0 0 .0 34048 .0 272640 .0 194699 .7 1756416 .0 181419 .9
     18304 .0 17865 .1 2688 .0 2497 .6 3  0 .275 0  0 .000 0 .275
    6 .7 34048 .0 34048 .0 34048 .0 0 .0 272640 .0 247555 .4 1756416 .0 263447 .9
     18816 .0 18123 .3 2688 .0 2523 .1 4  0 .359 0  0 .000 0 .359
    7 .7 34048 .0 34048 .0 0 .0 34048 .0 272640 .0 257729 .3 1756416 .0 345109 .8
     19072 .0 18396 .6 2688 .0 2550 .3 5  0 .451 0  0 .000 0 .451
    8 .7 34048 .0 34048 .0 34048 .0 34048 .0 272640 .0 272640 .0 1756416 .0 444982 .5
     19456 .0 18681 .3 2816 .0 2575 .8 7  0 .550 0  0 .000 0 .550
    9 .7 34048 .0 34048 .0 34046 .7 0 .0 272640 .0 16777 .0 1756416 .0 587906 .3
     20096 .0 19235 .1 2944 .0 2631 .8 8  0 .720 0  0 .000 0 .720
    10 .7 34048 .0 34048 .0 0 .0 34046 .2 272640 .0 80171 .6 1756416 .0 664913 .4
     20352 .0 19495 .9 2944 .0 2657 .4 9  0 .810 0  0 .000 0 .810
    11 .7 34048 .0 34048 .0 34048 .0 0 .0 272640 .0 129480 .8 1756416 .0 745100 .2
     20608 .0 19704 .5 2944 .0 2678 .4 10  0 .896 0  0 .000 0 .896
    12 .7 34048 .0 34048 .0 0 .0 34046 .6 272640 .0 164070 .7 1756416 .0 822073 .7
     20992 .0 19937 .1 3072 .0 2702 .8 11  0 .978 0  0 .000 0 .978
    13 .7 34048 .0 34048 .0 34048 .0 0 .0 272640 .0 211949 .9 1756416 .0 897364 .4
     21248 .0 20179 .6 3072 .0 2728 .1 12  1 .087 1  0 .004 1 .091
    14 .7 34048 .0 34048 .0 0 .0 34047 .1 272640 .0 245801 .5 1756416 .0 597362 .6
     21504 .0 20390 .6 3072 .0 2750 .3 13  1 .183 2  0 .050 1 .233
    15 .7 34048 .0 34048 .0 0 .0 34048 .0 272640 .0 21474 .1 1756416 .0 757347 .0
     22012 .0 20792 .0 3200 .0 2791 .0 15  1 .336 2  0 .050 1 .386
    16 .7 34048 .0 34048 .0 34047 .0 0 .0 272640 .0 48378 .0 1756416 .0 838594 .4
     22268 .0 21003 .5 3200 .0 2813 .2 16  1 .433 2  0 .050 1 .484
    
    3 .157: [GC (Allocation Failure) 3 .157: [ParNew: 272640K->34048K(306688K),
    0 .0844702 secs] 272640K->69574K(2063104K), 0 .0845560 secs] [Times: user= 0 .23
    sys= 0 .03, real= 0 .09 secs]
    4 .092: [GC (Allocation Failure) 4 .092: [ParNew: 306688K->34048K(306688K),
    0 .1013723 secs] 342214K->136584K(2063104K), 0 .1014307 secs] [Times: user= 0 .25
    sys= 0 .05, real= 0 .10 secs]
    ... cut for brevity ...
    11 .292: [GC (Allocation Failure) 11 .292: [ParNew: 306686K->34048K(306688K),
    0 .0857219 secs] 971599K->779148K(2063104K), 0 .0857875 secs] [Times: user= 0 .26
    sys= 0 .04, real= 0 .09 secs]
    12 .140: [GC (Allocation Failure) 12 .140: [ParNew: 306688K->34046K(306688K),
    0 .0821774 secs] 1051788K->856120K(2063104K), 0 .0822400 secs] [Times: user= 0 .25
    sys= 0 .03, real= 0 .08 secs]
    12 .989: [GC (Allocation Failure) 12 .989: [ParNew: 306686K->34048K(306688K),
    0 .1086667 secs] 1128760K->931412K(2063104K), 0 .1087416 secs] [Times: user= 0 .24
    sys= 0 .04, real= 0 .11 secs]
    13 .098: [GC (CMS Initial Mark) [1 CMS-initial-mark: 897364K(1756416K)]
    936667K(2063104K), 0 .0041705 secs] [Times: user= 0 .02 sys= 0 .00, real= 0 .00 secs]
    13 .102: [CMS-concurrent-mark-start]
    13 .341: [CMS-concurrent-mark: 0 .238/0.238 secs] [Times: user= 0 .36 sys= 0 .01,
    real= 0 .24 secs]
    13 .341: [CMS-concurrent-preclean-start]
    13 .350: [CMS-concurrent-preclean: 0 .009/0.009 secs] [Times: user= 0 .03 sys= 0 .00,
    real= 0 .01 secs]
    13 .350: [CMS-concurrent-abortable-preclean-start]
    13 .878: [GC (Allocation Failure) [ParNew: 16844397K->85085K(18874368K),
    0 .0960456 secs]
    
    1. ParNew: 16844397K->85085K(18874368K), 0.0960456 secs
    其中，16844397K表示GC前的新生代占用量，85085K表示GC后的新生代占用量，GC后Eden和
    一个Survivor为空，所以85085K也是另一个Survivor的占用量。括号中的18874368K是Eden+一
    个被占用Survivor的总和（18g）。
    2. 116885867K->100127390K(155189248K), 0.0961542 secs
    其中，分别是Java堆在垃圾回收前后的大小，和Java堆大小。说明堆使用为
    116885867K=111.47g，回收大小为100127390K=95.49g，堆大小为155189248K=148g（去掉
    其中一个Survivor），回收了16g空间.


总结：

在HDFS Namenode内存中的对象大都是文件，目录和blocks，这些数据只要不被程序或者数据的拥
有者人为的删除，就会在Namenode的运 行生命期内一直存在，所以这些对象通常是存在在old区中，
所以，如果整个hdfs文件和目录数多，blocks数也多，内存数据也会很大，如何降低Full GC的影响？

    计算NN所需的内存大小，合理配置JVM

使用低卡顿G1收集器

为什么会有G1呢？

因为并发、并行和CMS垃圾收集器都有 2 个共同的问题：

    116885867K->100127390K(155189248K), 0 .0961542 secs] [Times: user= 0 .14 sys= 0 .00,
    real= 0 .05 secs]
    14 .366: [CMS-concurrent-abortable-preclean: 0 .917/1.016 secs] [Times: user= 2 .22
    sys= 0 .07, real= 1 .01 secs]
    14 .366: [GC (CMS Final Remark) [YG occupancy: 182593 K (306688 K)]14.366:
    [Rescan (parallel) , 0 .0291598 secs]14.395: [weak refs processing, 0 .0000232
    secs]14.395: [class unloading, 0 .0117661 secs]14.407: [scrub symbol table,
    0 .0015323 secs]14.409: [scrub string table, 0 .0003221 secs][1 CMS-remark:
    976591K(1756416K)] 1159184K(2063104K), 0 .0462010 secs] [Times: user= 0 .14
    sys= 0 .00, real= 0 .05 secs]
    14 .412: [CMS-concurrent-sweep-start]
    14 .633: [CMS-concurrent-sweep: 0 .221/0.221 secs] [Times: user= 0 .37 sys= 0 .00,
    real= 0 .22 secs]
    14 .633: [CMS-concurrent-reset-start]
    14 .636: [CMS-concurrent-reset: 0 .002/0.002 secs] [Times: user= 0 .00 sys= 0 .00,
    real= 0 .00 secs]

老年代收集器大部分操作都必须扫描 整个老年代空间 （标记，清除和压缩）。这就导致了GC随着

    Java堆空间而线性增加或减少
    年轻代和老年代是独立的连续内存块，所以要先决定年轻代和年老代放在虚拟地址空间的位置

G1垃圾收集器利用分而治之的思想将堆进行分区，划分为一个个的区域。

G1垃圾收集器将堆拆成一系列的分区，这样的话，大部分的垃圾收集操作就只在一个分区内执行，从而

避免很多GC操作在整个Java堆或者整个年轻代进行。

编辑hadoop-env.sh

注意： 如果现在采用的垃圾收集器没有问题，就不要选择G1，如果追求低停顿，可以尝试使用G1

第六节 Hadoop二次开发环境搭建

系统环境

准备工作

安装Maven

    export HADOOP_NAMENODE_OPTS="-server -Xmx220G -Xms200G -XX:+UseG1GC -
    XX:MaxGCPauseMillis=200 -XX:+UnlockExperimentalVMOptions -
    XX:+ParallelRefProcEnabled -XX:-ResizePLAB -XX:+PerfDisableSharedMem -XX:-
    OmitStackTraceInFastThrow -XX:G1NewSizePercent=2 -XX:ParallelGCThreads=23 -
    XX:InitiatingHeapOccupancyPercent=40 -XX:G1HeapRegionSize=32M -
    XX:G1HeapWastePercent=10 -XX:G1MixedGCCountTarget=16 -verbose:gc -
    XX:+PrintGCDetails -XX:+PrintGCDateStamps -XX:+PrintGCTimeStamps -
    XX:+UseGCLogFileRotation -XX:NumberOfGCLogFiles=5 -XX:GCLogFileSize=100M -
    Xloggc:/var/log/hbase/gc.log -Dhadoop.security.logger=${HADOOP_SECURITY_LOGGER:-
    INFO,RFAS} -Dhdfs.audit.logger=${HDFS_AUDIT_LOGGER:-INFO,NullAppender}
    $HADOOP_NAMENODE_OPTS"
    
    系统： CentOS-7_x86_64
    protobuf: protoc-2.5.0
    maven： maven-3.6.0
    hadoop: hadoop-2.9.2
    java： jdk1.8.0_131
    cmake: cmake-2.8.12.2
    OpenSSL: OpenSSL 1 .0.2k-fips
    findbugs： findbugs-1.3.9

# 安装编译需要的依赖库

    yum install -y lzo-devel zlib-devel autoconf automake libtool cmake openssl-
    devel cmake gcc gcc-c++
    
    #上传maven安装包
    # 解压缩
    $ tar -zxvf apache-maven-3.6.3-bin.tar.gz -C /usr/local/

安装protobuf

# 配置到系统环境变量

    $ vim /etc/profile
    
    export MAVEN_HOME=/usr/local/apache-maven-3.6.3
    export PATH=$PATH:$MAVEN_HOME/bin
    
    # 刷新配置文件
    $ source /etc/profile
    
    # 验证maven安装是是否成功
    $ mvn -version
    
    [root@localhost ~]# mvn -version
    Apache Maven 3 .6.0 (ff8f5e7444045639af65f6095c62210b5713f426; 2018 -10-
    25T03:39:06+ 08 :00)
    Maven home: /usr/local/apache-maven-3.6.3
    Java version: 1 .8.0_131, vendor: Oracle Corporation
    Java home: /usr/local/jdk1.8.0_131/jre
    Default locale: en_US, platform encoding: UTF-8
    OS name: "linux", version: "4.20.13-1.el7.elrepo.x86_64", arch: "amd64", family:
    "unix"

# 安装依赖环境

    $ yum groupinstall Development tools -y
    
    # 下载
    $ https://github.com/protocolbuffers/protobuf/releases/download/v2.5.0/protobuf-
    2.5.0.tar.gz
    #上传protobuf安装包
    
    # 解压缩
    $ tar -zxvf protobuf-2.5.0.tar.gz
    cd protobuf-2.5.0
    
    # 进入解压目录 配置安装路径（--prefix=/usr/local/protobuf-2.5.0）
    $ ./configure --prefix=/usr/local/protobuf-2.5.0
    
    # 编译
    $ make
    
    # 验证编译文件
    $ make check
    
    # 安装
    $ make install
    
    # 配置protobuf环境变量
    $ vim /etc/profile
    
    export PROTOCBUF_HOME=/usr/local/protobuf-2.5.0
    export PATH=$PATH:$PROTOCBUF_HOME/bin
    
    # 刷新配置文件

安装Findbugs

添加aliyun镜像

找到maven环境下的settings.xml文件，添加镜像代理

上传源码文件

进入代码文件目标路径

    $ source /etc/profile
    
    # 验证是否安装成功
    $ protoc --version
    
    [root@localhost ~]# protoc --version
    libprotoc 2 .5.0

#下载

    $ https://jaist.dl.sourceforge.net/project/findbugs/findbugs/1.3.9/findbugs-
    1.3.9.tar.gz
    #上传安装包
    # 解压缩
    $ tar -zxvf findbugs-1.3.9.tar.gz -C /usr/local/
    
    # 配置系统环境变量
    $ vim /etc/profile
    
    export FINDBUGS_HOME=/usr/local/findbugs-1.3.9
    export PATH=$PATH:$FINDBUGS_HOME/bin
    
    # 刷新配置文件
    $ source /etc/profile
    
    # 验证是否安装成功
    $ findbugs -version
    
    [root@localhost ~]# findbugs -version
    1 .3.9
    
    <mirror>
    <id>nexus</id>
    <mirrorOf>*</mirrorOf>
    <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
    </mirror>
    <mirror>
    <id>nexus-public-snapshots</id>
    <mirrorOf>public-snapshots</mirrorOf>
    <url>http://maven.aliyun.com/nexus/content/repositories/snapshots/</url>
    </mirror>
    
    /root/hadoop-2.9.2-src/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-
    mapreduce-client-core/src/main/java/org/apache/hadoop/mapreduce/lib/input

编译

进入Hadoop源码目录

执行编译命令

问题解决

hadoop-aws:jar时缺少依赖包DynamoDBLocal:jar

选择手动下载该Jar包，上传到本地maven仓库

编译成功

    cd /root/hadoop-2.9.2-src
    
    mvn package -Pdist,native -DskipTests -Dtar
    
    cd /root/.m2/repository/com/amazonaws/DynamoDBLocal/1.11.86
    
    [INFO] Reactor Summary for Apache Hadoop Main 2.9.2:
    [INFO]
    [INFO] Apache Hadoop Main ................................. SUCCESS [ 1.165 s]
    [INFO] Apache Hadoop Build Tools .......................... SUCCESS [ 0.747 s]
    [INFO] Apache Hadoop Project POM .......................... SUCCESS [ 0.808 s]
    [INFO] Apache Hadoop Annotations .......................... SUCCESS [ 1.773 s]
    [INFO] Apache Hadoop Assemblies ........................... SUCCESS [ 0.202 s]
    [INFO] Apache Hadoop Project Dist POM ..................... SUCCESS [ 1.468 s]
    [INFO] Apache Hadoop Maven Plugins ........................ SUCCESS [ 2.847 s]
    [INFO] Apache Hadoop MiniKDC .............................. SUCCESS [ 3.001 s]
    [INFO] Apache Hadoop Auth ................................. SUCCESS [ 4.041 s]
    [INFO] Apache Hadoop Auth Examples ........................ SUCCESS [ 2.509 s]
    [INFO] Apache Hadoop Common ............................... SUCCESS [ 49.423 s]
    [INFO] Apache Hadoop NFS .................................. SUCCESS [ 3.809 s]
    [INFO] Apache Hadoop KMS .................................. SUCCESS [ 10.492 s]
    [INFO] Apache Hadoop Common Project ....................... SUCCESS [ 0.063 s]
    [INFO] Apache Hadoop HDFS Client .......................... SUCCESS [ 12.434 s]
    [INFO] Apache Hadoop HDFS ................................. SUCCESS [ 37.065 s]
    [INFO] Apache Hadoop HDFS Native Client ................... SUCCESS [ 1.951 s]
    [INFO] Apache Hadoop HttpFS ............................... SUCCESS [ 15.952 s]
    [INFO] Apache Hadoop HDFS BookKeeper Journal .............. SUCCESS [ 3.922 s]
    [INFO] Apache Hadoop HDFS-NFS ............................. SUCCESS [ 3.316 s]
    [INFO] Apache Hadoop HDFS-RBF ............................. SUCCESS [ 15.651 s]

[INFO] Apache Hadoop HDFS Project ......................... SUCCESS [ 0.041 s]
[INFO] Apache Hadoop YARN ................................. SUCCESS [ 0.050 s]
[INFO] Apache Hadoop YARN API ............................. SUCCESS [ 8.927 s]
[INFO] Apache Hadoop YARN Common .......................... SUCCESS [01:10 min]
[INFO] Apache Hadoop YARN Registry ........................ SUCCESS [ 4.114 s]
[INFO] Apache Hadoop YARN Server .......................... SUCCESS [ 0.040 s]
[INFO] Apache Hadoop YARN Server Common ................... SUCCESS [ 9.403 s]
[INFO] Apache Hadoop YARN NodeManager ..................... SUCCESS [ 18.255 s]
[INFO] Apache Hadoop YARN Web Proxy ....................... SUCCESS [ 2.929 s]
[INFO] Apache Hadoop YARN ApplicationHistoryService ....... SUCCESS [ 5.629 s]
[INFO] Apache Hadoop YARN Timeline Service ................ SUCCESS [ 4.198 s]
[INFO] Apache Hadoop YARN ResourceManager ................. SUCCESS [ 37.668 s]
[INFO] Apache Hadoop YARN Server Tests .................... SUCCESS [ 0.762 s]
[INFO] Apache Hadoop YARN Client .......................... SUCCESS [ 4.561 s]
[INFO] Apache Hadoop YARN SharedCacheManager .............. SUCCESS [ 3.043 s]
[INFO] Apache Hadoop YARN Timeline Plugin Storage ......... SUCCESS [ 2.427 s]
[INFO] Apache Hadoop YARN Router .......................... SUCCESS [ 3.277 s]
[INFO] Apache Hadoop YARN TimelineService HBase Backend ... SUCCESS [ 5.804 s]
[INFO] Apache Hadoop YARN Timeline Service HBase tests .... SUCCESS [ 1.815 s]
[INFO] Apache Hadoop YARN Applications .................... SUCCESS [ 0.040 s]
[INFO] Apache Hadoop YARN DistributedShell ................ SUCCESS [ 2.236 s]
[INFO] Apache Hadoop YARN Unmanaged Am Launcher ........... SUCCESS [ 1.769 s]
[INFO] Apache Hadoop YARN Site ............................ SUCCESS [ 0.045 s]
[INFO] Apache Hadoop YARN UI .............................. SUCCESS [ 0.046 s]
[INFO] Apache Hadoop YARN Project ......................... SUCCESS [ 6.043 s]
[INFO] Apache Hadoop MapReduce Client ..................... SUCCESS [ 0.180 s]
[INFO] Apache Hadoop MapReduce Core ....................... SUCCESS [ 59.165 s]
[INFO] Apache Hadoop MapReduce Common ..................... SUCCESS [ 28.713 s]
[INFO] Apache Hadoop MapReduce Shuffle .................... SUCCESS [ 3.509 s]
[INFO] Apache Hadoop MapReduce App ........................ SUCCESS [ 9.007 s]
[INFO] Apache Hadoop MapReduce HistoryServer .............. SUCCESS [ 4.384 s]
[INFO] Apache Hadoop MapReduce JobClient .................. SUCCESS [ 3.720 s]
[INFO] Apache Hadoop MapReduce HistoryServer Plugins ...... SUCCESS [ 1.733 s]
[INFO] Apache Hadoop MapReduce Examples ................... SUCCESS [ 3.492 s]
[INFO] Apache Hadoop MapReduce ............................ SUCCESS [ 3.083 s]
[INFO] Apache Hadoop MapReduce Streaming .................. SUCCESS [ 3.086 s]
[INFO] Apache Hadoop Distributed Copy ..................... SUCCESS [ 6.657 s]
[INFO] Apache Hadoop Archives ............................. SUCCESS [ 1.497 s]
[INFO] Apache Hadoop Archive Logs ......................... SUCCESS [ 1.641 s]
[INFO] Apache Hadoop Rumen ................................ SUCCESS [ 3.512 s]
[INFO] Apache Hadoop Gridmix .............................. SUCCESS [ 3.253 s]
[INFO] Apache Hadoop Data Join ............................ SUCCESS [ 1.906 s]
[INFO] Apache Hadoop Ant Tasks ............................ SUCCESS [ 1.669 s]
[INFO] Apache Hadoop Extras ............................... SUCCESS [ 2.252 s]
[INFO] Apache Hadoop Pipes ................................ SUCCESS [ 0.319 s]
[INFO] Apache Hadoop OpenStack support .................... SUCCESS [ 2.962 s]
[INFO] Apache Hadoop Amazon Web Services support .......... SUCCESS [ 7.011 s]
[INFO] Apache Hadoop Azure support ........................ SUCCESS [ 5.998 s]
[INFO] Apache Hadoop Aliyun OSS support ................... SUCCESS [ 4.510 s]
[INFO] Apache Hadoop Client ............................... SUCCESS [ 5.978 s]
[INFO] Apache Hadoop Mini-Cluster ......................... SUCCESS [ 0.536 s]
[INFO] Apache Hadoop Scheduler Load Simulator ............. SUCCESS [ 3.995 s]
[INFO] Apache Hadoop Resource Estimator Service ........... SUCCESS [ 4.019 s]
[INFO] Apache Hadoop Azure Data Lake support .............. SUCCESS [ 3.439 s]
[INFO] Apache Hadoop Tools Dist ........................... SUCCESS [ 18.642 s]
[INFO] Apache Hadoop Tools ................................ SUCCESS [ 0.038 s]
[INFO] Apache Hadoop Distribution ......................... SUCCESS [ 47.384 s]
[INFO] Apache Hadoop Cloud Storage ........................ SUCCESS [ 2.062 s]

[INFO] Apache Hadoop Cloud Storage Project ................ SUCCESS [ 0.034 s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------